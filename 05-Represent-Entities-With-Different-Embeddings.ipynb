{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"3nHtYS16HN8s","executionInfo":{"status":"ok","timestamp":1714463623150,"user_tz":420,"elapsed":1834,"user":{"displayName":"Vanessa","userId":"09973897643551096523"}}},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","from gensim.models import Word2Vec, FastText\n","#import glove\n","#from glove import Corpus\n","\n","import collections\n","import gc\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"eaXWFg5xRP68"},"source":["### Process NER clinical notes"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"-Q5J-bJQRP69","colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"status":"error","timestamp":1714463665014,"user_tz":420,"elapsed":218,"user":{"displayName":"Vanessa","userId":"09973897643551096523"}},"outputId":"32554f44-c60e-4b68-98ca-4621ef0c4ed0"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'data/ner_df.p'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-2a311ec8160a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/ner_df.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \"\"\"\n\u001b[1;32m    178\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/ner_df.p'"]}],"source":["# Load data\n","new_notes = pd.read_pickle(\"data/ner_df.p\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RmQdN4XQHeAu","colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"status":"error","timestamp":1714463625638,"user_tz":420,"elapsed":3,"user":{"displayName":"Vanessa","userId":"09973897643551096523"}},"outputId":"43a81b55-3cad-4eab-d6b7-f04902e41ac0"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'new_notes' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-90815b83fc9e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Filter out rows with empty 'ner' values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnull_index_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_notes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_notes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnull_index_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'new_notes' is not defined"]}],"source":["# Filter out rows with empty 'ner' values\n","null_index_list = [i.Index for i in new_notes.itertuples() if len(i.ner) == 0]\n","new_notes.drop(null_index_list, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CE3iEeTgHflr"},"outputs":[],"source":["# Process NER data\n","med7_ner_data = {}\n","for ii in new_notes.itertuples():\n","    p_id = ii.SUBJECT_ID\n","    ind = ii.Index\n","\n","    try:\n","        new_ner = new_notes.loc[ind].ner\n","    except:\n","        new_ner = []\n","\n","    new_temp = [(k[0], k[1]) for j in new_ner for k in j]\n","\n","    if p_id in med7_ner_data:\n","        med7_ner_data[p_id].extend(new_temp)\n","    else:\n","        med7_ner_data[p_id] = new_temp\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODo5KNAdHhdF"},"outputs":[],"source":["# Save processed NER data\n","pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iqf2mXC7RP69"},"outputs":[],"source":["data_types = [med7_ner_data]\n","data_names = [\"new_ner\"]"]},{"cell_type":"markdown","metadata":{"id":"GVqMyGsNRP69"},"source":["### Represent medical entites with Word2Vec embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgoTWjPQHcvw"},"outputs":[],"source":["# Load data\n","w2vec = Word2Vec.load(\"embeddings/word2vec.model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtLIccAaHjmD"},"outputs":[],"source":["# Calculate mean for vectors\n","def mean(a):\n","    return sum(a) / len(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZHfO9OOAHlCj","outputId":"057d76c8-b335-4bf7-e4e0-d5b47e05bc8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["w2vec starting..\n","Number of Word2Vec embeddings: 31732\n"]}],"source":["# Process and save Word2Vec embeddings\n","\n","for data, names in zip(data_types, data_names):\n","    new_word2vec = {}\n","    print(\"w2vec starting..\")\n","    for k, v in data.items():\n","        patient_temp = []\n","        if isinstance(v, list):\n","            for i in v:\n","                if isinstance(i, tuple) and len(i) == 2 and isinstance(i[0], str):\n","                    if i[0] in w2vec.wv:\n","                        patient_temp.append(w2vec.wv[i[0]])\n","                    elif len(i[0].split(\" \")) > 1:\n","                        avg = []\n","                        words = i[0].split(\" \")\n","                        num = 0\n","                        for each_word in words:\n","                            if each_word in w2vec.wv:\n","                                temp = w2vec.wv[each_word]\n","                                avg.append(temp)\n","                                num += 1\n","                        if num > 0:\n","                            avg = np.asarray(avg)\n","                            t = np.asarray(list(map(mean, zip(*avg))))\n","                            patient_temp.append(t)\n","        if patient_temp:\n","            new_word2vec[k] = patient_temp\n","\n","    print(f\"Number of Word2Vec embeddings: {len(new_word2vec)}\")\n","    pd.to_pickle(new_word2vec, \"data/\"+names+\"_word2vec_dict.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"Q33Rl99SRP6_"},"source":["### Represent medical entites with FastText embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhB8bS6URP6_"},"outputs":[],"source":["# Load data\n","fasttext = FastText.load(\"embeddings/fasttext.model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xyn1jYcxRP6_","outputId":"df309530-a84f-4f75-d854-8bc43f57ee32"},"outputs":[{"name":"stdout","output_type":"stream","text":["fasttext starting..\n","Number of Fasttext embeddings: 31461\n"]}],"source":["# Process and save FastText embeddings\n","\n","for data, names in zip(data_types, data_names):\n","    new_fasttextvec = {}\n","    print(\"fasttext starting..\")\n","\n","    for k,v in data.items():\n","        patient_temp = []\n","        for i in v:\n","            try:\n","                patient_temp.append(fasttext.wv[i[0]])\n","            except:\n","                pass\n","        if len(patient_temp) == 0: continue\n","        new_fasttextvec[k] = patient_temp\n","\n","    print(f\"Number of Fasttext embeddings: {len(new_fasttextvec)}\")\n","    pd.to_pickle(new_fasttextvec, \"data/\"+names+\"_fasttext_dict.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"n4fEWm5cRP6_"},"source":["### Represent medical entites with combined Word2Vec + FastText embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnmZSk3ARP6_","outputId":"d804aa6e-369b-4b80-ee36-4558679045e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["combined starting..\n","Number of concatenated embeddings: 32108\n"]}],"source":["# Process and save concatenated embeddings\n","\n","for data, names in zip(data_types, data_names):\n","    print(\"combined starting..\")\n","    new_concatvec = {}\n","\n","    for k,v in data.items():\n","        patient_temp = []\n","    #     if k != 6: continue\n","        for i in v:\n","            w2vec_temp = []\n","            try:\n","                w2vec_temp = w2vec.wv[i[0]]\n","            except:\n","                avg = []\n","                num = 0\n","                temp = []\n","\n","                if len(i[0].split(\" \")) > 1:\n","                    for each_word in i[0].split(\" \"):\n","                        try:\n","                            temp = w2vec.wv[each_word]\n","                            avg.append(temp)\n","                            num += 1\n","                        except:\n","                            pass\n","                    if num == 0:\n","                        w2vec_temp = [0] * 100\n","                    else:\n","                        #print(f\"i: {i[0]}\")\n","                        avg = np.asarray(avg)\n","                        #w2vec_temp = np.asarray(map(mean, zip(*avg)))\n","                        w2vec_temp = np.mean(avg, axis=0)\n","                        #print(f\"w2v: {w2vec_temp}\")\n","                else:\n","                    w2vec_temp = [0] * 100\n","            try:\n","                fasttemp = fasttext.wv[i[0]]\n","            except:\n","                fasttemp = [0] * 100\n","\n","            #print(f\"i[0]: {i[0]}\")\n","            #print(f\"Length of w2v: {len(w2vec_temp)}\")\n","            #print(f\"Length of fasttext: {len(fasttemp)}\")\n","            appended = np.append(fasttemp, w2vec_temp, 0)\n","            patient_temp.append(appended)\n","        if len(patient_temp) == 0: continue\n","        new_concatvec[k] = patient_temp\n","\n","    print(f\"Number of concatenated embeddings: {len(new_concatvec)}\")\n","    pd.to_pickle(new_concatvec, \"data/\"+names+\"_combined_dict.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"j5-hPqA2RP6_"},"source":["### Standardize all 3 embeddings\n","Remove key-value pairs from fasttext embeddings and combined embeddings for keys that are not present in word2vec embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeMnqLJjRP6_"},"outputs":[],"source":["new_fasttext_dict = new_fasttextvec.copy()\n","new_word2vec_dict =  new_word2vec.copy()\n","new_combined_dict = new_concatvec.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OBSQIOitRP6_","outputId":"bf2dd315-62fa-4a78-d8b8-5c746d35fcde"},"outputs":[{"name":"stdout","output_type":"stream","text":["31732 31461 32108\n"]}],"source":["diff = set(new_fasttext_dict.keys()).difference(set(new_word2vec_dict))\n","for i in diff:\n","    del new_fasttext_dict[i]\n","    del new_combined_dict[i]\n","print (len(new_word2vec_dict), len(new_fasttext_dict), len(new_combined_dict))\n","\n","pd.to_pickle(new_word2vec_dict, \"data/\"+\"new_ner\"+\"_word2vec_limited_dict.pkl\")\n","pd.to_pickle(new_fasttext_dict, \"data/\"+\"new_ner\"+\"_fasttext_limited_dict.pkl\")\n","pd.to_pickle(new_combined_dict, \"data/\"+\"new_ner\"+\"_combined_limited_dict.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"topcfDK6RP7A"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}