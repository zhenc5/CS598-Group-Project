{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf9ElKlnpiF_"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, FastText\n",
        "#import glove\n",
        "#from glove import Corpus\n",
        "\n",
        "import collections\n",
        "import gc\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, LSTM, GRU\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
        "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras.backend import set_session, clear_session, get_session\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjFl-9vOpiGA"
      },
      "outputs": [],
      "source": [
        "# import medical entities dataset\n",
        "med7_ner_data = pd.read_pickle(\"data/new_ner_word_dict.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk5EAyK6piGA"
      },
      "source": [
        "#### Exploring the medical entities dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9mISZTvpiGI"
      },
      "outputs": [],
      "source": [
        "# Get the unique entities of med7 model\n",
        "\n",
        "unique_categories = set()\n",
        "\n",
        "for values in med7_ner_data.values():\n",
        "    for item in values:\n",
        "        category = item[1]\n",
        "        unique_categories.add(category)\n",
        "\n",
        "print(f\"Unique words (categories) in the values: {unique_categories}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz0fDY0RpiGJ"
      },
      "outputs": [],
      "source": [
        "# Print the total counts for each med7 entity\n",
        "\n",
        "counts = {category: 0 for category in unique_categories}\n",
        "\n",
        "for values in med7_ner_data.values():\n",
        "    for item in values:\n",
        "        category = item[1]\n",
        "        counts[category] += 1\n",
        "\n",
        "print(\"Total count for each category:\")\n",
        "for category, count in counts.items():\n",
        "    print(f\"Total counts of {category}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WudkHzWBpiGJ"
      },
      "outputs": [],
      "source": [
        "# Print the unique counts for each med7 entity and an example\n",
        "\n",
        "unique_values_per_category = {category: set() for category in unique_categories}\n",
        "\n",
        "for values in med7_ner_data.values():\n",
        "    for item in values:\n",
        "        value, category = item\n",
        "        unique_values_per_category[category].add(value)\n",
        "\n",
        "print(\"Unique count for each category:\")\n",
        "for category, unique_values in unique_values_per_category.items():\n",
        "    print(f\"Unique counts of {category}: {len(unique_values)}\")\n",
        "    print(f\"{category} example: {list(unique_values)[12]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aApwOkmRpiGJ"
      },
      "outputs": [],
      "source": [
        "# Check if running code on CPU or GPU\n",
        "import tensorflow as tf\n",
        "\n",
        "# Enable device logging\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OePePYiHpiGJ"
      },
      "source": [
        "#### Timeseries model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-M7hY6BpiGJ"
      },
      "outputs": [],
      "source": [
        "# Helper functions for training the time series model\n",
        "\n",
        "def reset_keras(model):\n",
        "    \"\"\"reset keras Session\"\"\"\n",
        "    sess = get_session()\n",
        "    clear_session()\n",
        "    sess.close()\n",
        "    sess = get_session()\n",
        "\n",
        "    try:\n",
        "        del model # this is from global space - change this as you need\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    gc.collect() # if it's done something you should see a number being outputted\n",
        "\n",
        "def make_prediction_timeseries(model, test_data):\n",
        "    \"\"\"make model predictions\"\"\"\n",
        "    probs = model.predict(test_data)\n",
        "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
        "    return probs, y_pred\n",
        "\n",
        "def save_scores_timeseries(predictions, probs, ground_truth, model_name,\n",
        "                problem_type, iteration, hidden_unit_size, type_of_ner):\n",
        "    \"\"\"save metrics from model predictions\"\"\"\n",
        "    auc = roc_auc_score(ground_truth, probs)\n",
        "    auprc = average_precision_score(ground_truth, probs)\n",
        "    acc   = accuracy_score(ground_truth, predictions)\n",
        "    F1    = f1_score(ground_truth, predictions)\n",
        "\n",
        "    result_dict = {}\n",
        "    result_dict['auc'] = auc\n",
        "    result_dict['auprc'] = auprc\n",
        "    result_dict['acc'] = acc\n",
        "    result_dict['F1'] = F1\n",
        "\n",
        "    file_name = str(hidden_unit_size)+\"-\"+model_name+\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\".p\"\n",
        "\n",
        "    result_path = \"results/\"\n",
        "    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
        "\n",
        "    print(auc, auprc, acc, F1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4w0zS4OpiGK"
      },
      "outputs": [],
      "source": [
        "# Define the time series model\n",
        "\n",
        "def timeseries_model(layer_name, number_of_unit):\n",
        "    \"\"\"define time series model specifications\"\"\"\n",
        "    K.clear_session()\n",
        "\n",
        "    sequence_input = Input(shape=(24,104),  name = \"timeseries_input\")\n",
        "\n",
        "    if layer_name == \"LSTM\":\n",
        "        x = LSTM(number_of_unit)(sequence_input)\n",
        "    else:\n",
        "        x = GRU(number_of_unit)(sequence_input)\n",
        "\n",
        "    #logits_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
        "    logits_regularizer = keras.regularizers.l2(0.01)\n",
        "    sigmoid_pred = Dense(1, activation='sigmoid',use_bias=False,\n",
        "                         kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
        "                  kernel_regularizer=logits_regularizer)(x)\n",
        "\n",
        "    model = Model(inputs=sequence_input, outputs=sigmoid_pred)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tUz8MippiGK"
      },
      "outputs": [],
      "source": [
        "# import dataset\n",
        "type_of_ner = \"new\"\n",
        "\n",
        "x_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\n",
        "x_dev_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\n",
        "x_test_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n",
        "\n",
        "y_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\n",
        "y_dev = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\n",
        "y_test = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjYjPdF4piGK"
      },
      "outputs": [],
      "source": [
        "# Train and test the time series model\n",
        "epoch_num = 100\n",
        "model_patience = 3\n",
        "monitor_criteria = 'val_loss'\n",
        "batch_size = 128\n",
        "\n",
        "unit_sizes = [256]\n",
        "iter_num = 11\n",
        "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
        "layers = [\"GRU\"]\n",
        "for each_layer in layers:\n",
        "    print(\"Layer: \", each_layer)\n",
        "    for each_unit_size in unit_sizes:\n",
        "        print(\"Hidden unit: \", each_unit_size)\n",
        "        for iteration in range(1, iter_num):\n",
        "            print(\"Iteration number: \", iteration)\n",
        "            print(\"=============================\")\n",
        "\n",
        "            for each_problem in target_problems:\n",
        "                print (\"Problem type: \", each_problem)\n",
        "                print (\"__________________\")\n",
        "\n",
        "\n",
        "                early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n",
        "                best_model_name = str(each_layer)+\"-\"+str(each_unit_size)+\"-\"+str(each_problem)+\"-\"+\"best_model.keras\"\n",
        "                checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,\n",
        "                    save_best_only=True, mode='min')\n",
        "\n",
        "\n",
        "                callbacks = [early_stopping_monitor, checkpoint]\n",
        "\n",
        "                model = timeseries_model(each_layer, each_unit_size)\n",
        "                model.fit(x_train_lstm, y_train[each_problem], epochs=epoch_num, verbose=1,\n",
        "                          validation_data=(x_dev_lstm, y_dev[each_problem]), callbacks=callbacks, batch_size= batch_size)\n",
        "\n",
        "                model.load_weights(best_model_name)\n",
        "\n",
        "                probs, predictions = make_prediction_timeseries(model, x_test_lstm)\n",
        "                save_scores_timeseries(predictions, probs, y_test[each_problem].values,str(each_layer),\n",
        "                                       each_problem, iteration, each_unit_size,type_of_ner)\n",
        "                reset_keras(model)\n",
        "                #del model\n",
        "                clear_session()\n",
        "                gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lavd5HSfpiGK"
      },
      "source": [
        "#### Show average over iterations for each metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4sibXfTpiGK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Define categories and metrics\n",
        "categories = [\"128-GRU\", \"256-GRU\", \"128-LSTM\", \"256-LSTM\"]\n",
        "metrics = {\"auc\":\"AUROC\", \"auprc\":\"AUPRC\", \"acc\":\"Accuracy\", \"F1\":\"F1\"}\n",
        "tasks = [\"mort_hosp\", \"mort_icu\", \"los_3\", \"los_7\"]\n",
        "\n",
        "# Initialize dictionaries to store results\n",
        "results = {category: {task: {metric_name: [] for metric, metric_name in metrics.items()} for task in tasks} for category in categories}\n",
        "\n",
        "# Directory where pickle files are stored\n",
        "directory = \"results/\"\n",
        "\n",
        "# Loop through each file\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".p\"):\n",
        "        parts = filename.split(\"-\")\n",
        "        category = parts[0] + \"-\" + parts[1]\n",
        "        task = parts[2]\n",
        "        if category in categories and task in tasks:\n",
        "            result_dict = pd.read_pickle(os.path.join(directory, filename))\n",
        "            for metric, metric_name in metrics.items():\n",
        "                results[category][task][metric_name].append(result_dict[metric])\n",
        "\n",
        "# Calculate average and standard deviation\n",
        "\"\"\"\n",
        "for category in categories:\n",
        "    print(f\"Category: {category}\")\n",
        "    for task in tasks:\n",
        "        print(f\"Task: {task}\")\n",
        "        for metric in metrics:\n",
        "            values = results[category][task][metric]\n",
        "            avg = np.mean(values)\n",
        "            std = np.std(values)\n",
        "            print(f\"  {metric}: Avg = {avg}, Std = {std}\")\n",
        "\"\"\"\n",
        "\n",
        "for category in categories:\n",
        "    print(f\"Category: {category}\")\n",
        "    df_data = {task: {} for task in tasks}\n",
        "    for task in tasks:\n",
        "        task_data = {}\n",
        "        for metric, metric_name in metrics.items():\n",
        "            values = results[category][task][metric_name]\n",
        "            mean = np.mean(values)\n",
        "            std = np.std(values)\n",
        "            task_data[metric_name] = f\"{mean:.4f} \\u00B1 {std:.4f}\"\n",
        "        df_data[task] = task_data\n",
        "    df = pd.DataFrame(df_data).transpose()\n",
        "    print(df)\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}