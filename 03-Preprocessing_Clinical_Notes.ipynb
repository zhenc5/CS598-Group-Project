{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Helper functions for preprocessing text in the data file\n",
        "# This code is taken from https://github.com/kaggarwal/ClinicalNotesICU\n",
        "\n",
        "SECTION_TITLES = re.compile(\n",
        "    r'('\n",
        "    r'ABDOMEN AND PELVIS|CLINICAL HISTORY|CLINICAL INDICATION|COMPARISON|COMPARISON STUDY DATE'\n",
        "    r'|EXAM|EXAMINATION|FINDINGS|HISTORY|IMPRESSION|INDICATION'\n",
        "    r'|MEDICAL CONDITION|PROCEDURE|REASON FOR EXAM|REASON FOR STUDY|REASON FOR THIS EXAMINATION'\n",
        "    r'|TECHNIQUE'\n",
        "    r'):|FINAL REPORT',\n",
        "    re.I | re.M)\n",
        "\n",
        "\n",
        "def getSentences(t):\n",
        "    return list(preprocess_mimic(t))\n",
        "\n",
        "def pattern_repl(matchobj):\n",
        "    \"\"\"\n",
        "    Return a replacement string to be used for match object\n",
        "    \"\"\"\n",
        "    return ' '.rjust(len(matchobj.group(0)))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean text\n",
        "    \"\"\"\n",
        "\n",
        "    # Replace [**Patterns**] with spaces.\n",
        "    text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', pattern_repl, text)\n",
        "    # Replace `_` with spaces.\n",
        "    text = re.sub(r'_', ' ', text)\n",
        "\n",
        "    start = 0\n",
        "    end = find_end(text)\n",
        "    new_text = ''\n",
        "    if start > 0:\n",
        "        new_text += ' ' * start\n",
        "    new_text = text[start:end]\n",
        "\n",
        "    # make sure the new text has the same length of old text.\n",
        "    if len(text) - end > 0:\n",
        "        new_text += ' ' * (len(text) - end)\n",
        "    return new_text\n",
        "\n",
        "def preprocess_mimic(text):\n",
        "    \"\"\"\n",
        "    Preprocess reports in MIMIC-III.\n",
        "    1. remove [**Patterns**] and signature\n",
        "    2. split the report into sections\n",
        "    3. tokenize sentences and words\n",
        "    4. lowercase\n",
        "    \"\"\"\n",
        "    for sec in split_heading(clean_text(text)):\n",
        "        for sent in sent_tokenize(sec):\n",
        "            text = ' '.join(word_tokenize(sent))\n",
        "            yield text.lower()\n",
        "\n",
        "def split_heading(text):\n",
        "    \"\"\"Split the report into sections\"\"\"\n",
        "    start = 0\n",
        "    for matcher in SECTION_TITLES.finditer(text):\n",
        "        # add last\n",
        "        end = matcher.start()\n",
        "        if end != start:\n",
        "            section = text[start:end].strip()\n",
        "            if section:\n",
        "                yield section\n",
        "\n",
        "        # add title\n",
        "        start = end\n",
        "        end = matcher.end()\n",
        "        if end != start:\n",
        "            section = text[start:end].strip()\n",
        "            if section:\n",
        "                yield section\n",
        "\n",
        "        start = end\n",
        "\n",
        "    # add last piece\n",
        "    end = len(text)\n",
        "    if start < end:\n",
        "        section = text[start:end].strip()\n",
        "        if section:\n",
        "            yield section\n",
        "\n",
        "def find_end(text):\n",
        "    \"\"\"Find the end of the report.\"\"\"\n",
        "    ends = [len(text)]\n",
        "    patterns = [\n",
        "        re.compile(r'BY ELECTRONICALLY SIGNING THIS REPORT', re.I),\n",
        "        re.compile(r'\\n {3,}DR.', re.I),\n",
        "        re.compile(r'[ ]{1,}RADLINE ', re.I),\n",
        "        re.compile(r'.*electronically signed on', re.I),\n",
        "        re.compile(r'M\\[0KM\\[0KM')\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        matchobj = pattern.search(text)\n",
        "        if matchobj:\n",
        "            ends.append(matchobj.start())\n",
        "    return min(ends)"
      ],
      "metadata": {
        "id": "AY45P3n4NKnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = '/content/drive/MyDrive/CS598_Project/data'\n",
        "MIMIC_EXTRACT_DATA = '/content/drive/MyDrive/CS598_Project/data/all_hourly_data.h5'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  admission_df = pd.read_csv(os.path.join(raw_data_dir, 'ADMISSIONS.csv'))\n",
        "  events_df = pd.read_csv(os.path.join(raw_data_dir, 'NOTEEVENTS.csv'), low_memory = False)\n",
        "  icu_stay_df = pd.read_csv(os.path.join(raw_data_dir, 'ICUSTAYS.csv'))\n",
        "\n",
        "  #print(len(admission_df))\n",
        "  #print(len(events_df))\n",
        "  #print(len(icu_stay_df))\n",
        "\n",
        "  return [admission_df, events_df, icu_stay_df]\n",
        "\n",
        "raw_data = load_raw_data(raw_data_dir)\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(raw_data):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "  admission_data = raw_data[0]\n",
        "  note_events_data = raw_data[1]\n",
        "  icu_stay_data = raw_data[2]\n",
        "  print(\"There are total \", len(admission_data), \" Admission Data\")\n",
        "  print(\"There are total \", len(icu_stay_data), \" ICU Stay Data\")\n",
        "\n",
        "  ## Select Clinical Notes\n",
        "  note_events_data.groupby(note_events_data.CATEGORY).agg(['count'])\n",
        "  note_categories = note_events_data.groupby(note_events_data.CATEGORY).agg(['count']).index\n",
        "  selected_note = []\n",
        "  for category in list(note_categories):\n",
        "    if category != 'Discharge summary':\n",
        "      selected_note.append(category)\n",
        "  ## Create sub notes based on category\n",
        "  sub_notes = note_events_data[note_events_data.CATEGORY.isin(selected_note)]\n",
        "  sub_notes.shape\n",
        "  ## - Handle missing chart\n",
        "  missing_chart_notes_idx = []\n",
        "  for note in sub_notes.itertuples():\n",
        "    if isinstance(note.CHARTTIME, str):\n",
        "      continue\n",
        "    if np.isnan(note.CHARTTIME):\n",
        "      missing_chart_notes_idx.append(note.Index)\n",
        "  print(\"{} of notes missing charttime.\".format(len(missing_chart_notes_idx)))\n",
        "  sub_notes.drop(missing_chart_notes_idx, inplace = True)\n",
        "  print(\"sub_notes shape: \", sub_notes.shape)\n",
        "\n",
        "  ## - Select based on Patient_ID\n",
        "  #notes_patient_id = sub_notes[sub_notes.SUBJECT_ID.isin(patient_ids)]\n",
        "  #sub_notes.shape\n",
        "  #print(len(note_events_data))\n",
        "\n",
        "  ## - Select based on Time Limit\n",
        "  statistic = pd.read_hdf(MIMIC_EXTRACT_DATA, 'patients')\n",
        "  print(\"MIMIC-EXTRACT DATA (Patients Num & Hospital Admission & ICU Admission): \", len(statistic))\n",
        "  TIMELIMIT = 1 ## 1 day\n",
        "  statistic.shape\n",
        "  statistic.head()\n",
        "  new_stats = statistic.reset_index()\n",
        "  new_stats.rename(columns = {\"subject_id\": \"SUBJECT_ID\", \"hadm_id\": \"HADM_ID\"}, inplace = True)\n",
        "  print(\"new_stats shape: \", new_stats.shape, \"\\nsub_notes shape: \", sub_notes.shape)\n",
        "  df_adm_notes = pd.merge(sub_notes[['ROW_ID','SUBJECT_ID','HADM_ID','CHARTTIME', 'CATEGORY', 'TEXT']],\n",
        "                          new_stats[['SUBJECT_ID','HADM_ID','icustay_id','age','admittime','dischtime', 'deathtime', 'intime', 'outtime', 'los_icu', 'mort_icu', 'mort_hosp', 'hospital_expire_flag', 'hospstay_seq', 'max_hours']],\n",
        "                          on = ['SUBJECT_ID'],\n",
        "                          how = 'left')\n",
        "  df_adm_notes.head()\n",
        "  df_adm_notes['CHARTTIME'] = pd.to_datetime(df_adm_notes['CHARTTIME'])\n",
        "  df_less_n = df_adm_notes[((df_adm_notes['CHARTTIME'] - df_adm_notes['intime']).dt.total_seconds() / (24*60*60)) < TIMELIMIT]\n",
        "  print(\"df_less_n.shape: \", df_less_n.shape)\n",
        "  pd.to_pickle(df_less_n, os.path.join(raw_data_dir, 'sub_notes.p'))\n",
        "  return df_less_n\n",
        "\n",
        "state = calculate_stats(raw_data)\n",
        "\n",
        "## Helper function to process text\n",
        "\n",
        "# process raw data\n",
        "def process_data(raw_data):\n",
        "\n",
        "  # implement this function to process the data as you need\n",
        "  clinical_notes = pd.read_pickle(os.path.join(raw_data_dir, 'sub_notes.p'))\n",
        "  clinical_notes.shape\n",
        "\n",
        "  eliminate_notes = clinical_notes[clinical_notes.SUBJECT_ID.notnull()]\n",
        "  eliminate_notes = eliminate_notes[eliminate_notes.CHARTTIME.notnull()]\n",
        "  eliminate_notes = eliminate_notes[eliminate_notes.TEXT.notnull()]\n",
        "  eliminate_notes.shape\n",
        "\n",
        "  eliminate_notes = eliminate_notes[['SUBJECT_ID', 'HADM_ID_y', 'CHARTTIME', 'TEXT']]\n",
        "  eliminate_notes['preprocessed_text'] = None\n",
        "\n",
        "  for notes in eliminate_notes.itertuples():\n",
        "    text = notes.TEXT\n",
        "    eliminate_notes.at[notes.Index, 'preprocessed_text'] = getSentences(text)\n",
        "\n",
        "  pd.to_pickle(eliminate_notes, os.path.join(raw_data_dir, 'preprocessed_notes.p'))\n",
        "  print(\"Preprocessed Data: \", eliminate_notes.shape)\n",
        "  return eliminate_notes\n",
        "\n",
        "processed_data = process_data(raw_data)\n",
        "\n",
        "'''\n",
        " you can load the processed data directly\n",
        "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
        "def load_processed_data(raw_data_dir):\n",
        "  pass\n",
        "\n",
        "'''"
      ]
    }
  ]
}
