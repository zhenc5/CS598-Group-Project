{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Reproducing the Paper \"Improving clinical outcome predictions using convolution over medical entities with multimodal learning\"\n",
        "\n",
        "Team 34: Kristine Cheng (cycheng4), Vanessa Chen (zhenc5), Sophia Yu (sophiay3)"
      ],
      "metadata": {
        "id": "tjoROCQXbaoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Location\n",
        "* Google Drive Link: https://drive.google.com/drive/folders/1nlDMRbCBY27ygu5EKwvnyUR3SDempmbJ?usp=drive_link\n",
        "\n",
        "* Github Link: https://github.com/zhenc5/CS598-Group-Project\n"
      ],
      "metadata": {
        "id": "mrHvwHU6Vl4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03263d4e-543d-4918-f48c-6daf8d91bd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "*   Background of the problem\n",
        "\n",
        "  It is crucial to assess a patientâ€™s health by looking at their medical tests and predicting how they might fare during their stay in the ICU. The type of problem addressed in the paper is clinical outcome prediction, specifically predicting mortality (in-hospital & in-ICU) and length of ICU stay (LOS) (>3 days & >7 days).\n",
        "\n",
        "  ![task_prediction.png](https://drive.google.com/uc?export=view&id=1Mfvjp_7vDHGOAF5SqUyOnfMb2tQQuhUM)\n",
        "  \n",
        "  Predicting clinical outcomes is an important problem in healthcare, because it can help hospitals and healthcare providers reduce healthcare costs, improve patient outcomes by determining treatment methods, and optimize healthcare resource utilization. By accurately predicting LOS and mortality, healthcare providers can provide targeted interventions to those at high risk, thereby leading to better patient outcomes and more efficient use of hospital resources.\n",
        "\n",
        "  One of the major difficulties associated with predicting these clinical outcomes using electronic health records (EHR) is in standardizing the preprocessing steps, such as in the handling of missing data and outliers, unit conversions, and the transformation of raw data into usable features to be used in deep learning algorithms [1].  Additionally, previous studies that aim to predict these clinical outcomes have used only structured patient data, such as historical patient diagnoses (ICD codes) [5, 6], lab results and other measurements taken in the ICU [7-9]. To improve the accuracy of the predictions, unstructured clinical notes can be added to the deep learning model. However, extracting medical entities from unstructured clinical notes presents its own challenges because it is free text usually containing grammatical errors, shorthand, medical jargon and redundant information [1].\n",
        "\n",
        "  Some state-of-the-art deep learning algorithm methods for using EHR data to predict clinical outcomes include Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) because they are most effective at learning from sequence data. Lipton et al. demonstrated the effectiveness of a LSTM to model clinical data, specifically to classify 128 diagnoses using 13 clinical measurements [10]. Choi et al. showed promising results in predicting multi-label diagnosis for a patient's next visit using a GRU-based model called DoctorAI [5].\n",
        "\n",
        "*   Paper explanation\n",
        "\n",
        "  Electronic Health Record (EHR) data is commonly used in deep learning applications for clinical outcome predictions. However, traditional approaches often overlook the unstructured data within EHR, such as clinical notes and radiology. Bardak and Tan address [1]  this issue by exploring methods to improve two different common risk prediction tasks - mortality and length of ICU stay (LOS). The paper proposes a deep learning method that involves extracting medical entities from clinical notes and integrating them into prediction models using a convolution-based multimodal architecture. Additionally, they evaluated different embedding techniques, such as Word2Vec and FastText on medical entities.\n",
        "\n",
        "  The innovative feature in the proposed method in the paper is the use of CNN architecture to capture local patterns in the EHR data and medical entity embeddings of the clinical notes, and then to combine the learned features from the CNN with features extracted from the timeseries data to make its predictions.\n",
        "\n",
        "  The results show that the proposed method outperforms the baseline models on all 4 clinical outcome predictions in terms of AUCROC, AURPRC, and F1 score, with the exception of LOS >7 days where the F1 score was greater for the baseline model.\n",
        "\n",
        "  Overall, the paper makes an important contribution to the research regime of clinical outcome predictions by introducing a novel approach that not only enhances the accuracy of these predictions but also has the adaptability to be applied to other clinical outcome prediction tasks. The implementation of convolution on medical entities, extracted from EHR clinical notes, in conjunction with multimodal learning, signifies an important step forward in the development of predictive models for clinical outcomes.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "Below lists the hypotheses to be tested and the corresponding experiments that will be run:\n",
        "  1. The proposed convolution-based multimodal architecture outperforms the baseline multimodal architecture for each of the 4 clinical outcome prediction tasks.\n",
        "  * Various embedding techniques, such as Word2Vec, FastText, and the concatenation of Word2Vec and FastText embeddings on the EHR clinical notes, will also be compared among the baseline multimodel architecture and the proposed convolution-based architecture.\n",
        "\n",
        "  2. The baseline multimodal model shows an improved prediction performance compared to the baseline time-series GRU and LSTM models on each of the 4 clinical outcome tasks.\n",
        "  * Various embedding techniques, such as Doc2Vec, Word2Vec, FastText, and the concatenation of Word2Vec and FastText embeddings on the EHR clinical notes, will also be compared among the baseline models.\n",
        "\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import the necessary packages"
      ],
      "metadata": {
        "id": "l0223qX6syge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "from nltk import sent_tokenize, word_tokenize, punkt\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "#import glove\n",
        "#from glove import Corpus\n",
        "\n",
        "import collections\n",
        "import gc\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, LSTM, GRU\n",
        "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, Convolution1D\n",
        "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D, MaxPool1D\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras.backend import set_session, clear_session, get_session\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
        "\n",
        "from logging import NullHandler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb40cfb-48de-4783-a7d2-c6979f453bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "In order to implement the paper's code, 3 folders must first be created in the directory and named \"data\", \"embeddings\", and \"results\".\n",
        "\n",
        "#### Links to the data sources:\n",
        "* Download the MIMIC-III dataset via https://mimic.physionet.org/\n",
        "* MIMIC-Extract implementation: https://github.com/MLforHealth/MIMIC_Extract\n",
        "* med7 implementation: https://github.com/kormilitzin/med7\n",
        "* Download Pre-trained Word2Vec embeddings: https://github.com/kexinhuang12345/clinicalBERT\n",
        "* Preprocessing Script: https://github.com/kaggarwal/ClinicalNotesICU\n",
        "\n",
        "#### Source of the data:\n",
        "\n",
        "The data is collected from running MIMIC-III data [2-4] through MIMIC-Extract Pipeline. Since we only wished to use the output of this pipeline, we were able to directly download a preprocessed version with default parameters from their Github page [4, 11]. The dataset is stored in the \"data\" folder as all_hourly_data.h5.\n",
        "\n",
        "\n",
        "#### Statistics:\n",
        "\n",
        "For the time series data:\n",
        "\n",
        "The MIMIC-III dataset contains EHR data of 58,976 unique hospital admissions and 61,532 ICU admissions from 46,520 patients.\n",
        "\n",
        "The MIMIC-Extract dataset contains a patient's first ICU visit and already eliminates patients with ages < 15 years and where the LOS is not between 12 hours and 10 days [1]. It contains 34,472 patients and 104 time-series variables.\n",
        "\n",
        "Then, we drop any patients who do not have at least 30 hours of data. We also drop any clinical notes that do not contain chart time information and any patients that do not have any clinical notes in 24 hours. This leads to a final cohort, after clinical note elimination, of 23,944 records of patients, hospital admissions, and ICU admissions.\n",
        "\n",
        "For the medical entities data:\n",
        "\n",
        "There will be 7 medical entities (Drug, Strength, Form, Route, Dosage, Frequency, Duration) with the final unique counts being 18268, 10749, 597, 1193, 7239, 3344, and 1185 respectfully.\n",
        "\n",
        "#### Data process:\n",
        "\n",
        "By feeding data through first 24 hour features, the data should be split into three different csv files named ADMISSION, NOTEEVENTS, ICUSTAYS respectfully and placed in the \"data\" folder.\n",
        "\n",
        "The medical entities from the clinical notes will be used to enhance the prediction performance. In order to extract the medical embeddings, we used a pre-trained clinical named-entity recognition (NER) model, med7 [12], which extracts 7 different entities (Drug, Strength, Duration, Route, Form, Dosage, Frequency). Then, we used the pre-trained Word2Vec and FastText embedding techniques [13] (stored in the \"embeddings\" folder) to convert the medical entities into word representations.\n",
        "\n",
        "The train/valid/test split, for all clinical tasks, is based on class\n",
        "distribution with 70%/10%/20% ratio.\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Time Series Data and Collect the Patient ID"
      ],
      "metadata": {
        "id": "WuM4OJIcOnuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/drive/MyDrive/CS598_Project/data'\n",
        "lvl2_train_imputer = pd.read_pickle(os.path.join(DATAPATH, \"lvl2_imputer_train.pkl\"))\n",
        "lvl2_dev_imputer = pd.read_pickle(os.path.join(DATAPATH, \"lvl2_imputer_dev.pkl\"))\n",
        "lvl2_test_imputer = pd.read_pickle(os.path.join(DATAPATH,\"lvl2_imputer_test.pkl\"))\n",
        "Ys = pd.read_pickle(os.path.join(DATAPATH, \"Ys.pkl\"))\n",
        "\n",
        "print(\"Shape of train, dev, test {}, {}, {}.\".format((lvl2_train_imputer.shape), (lvl2_dev_imputer.shape), (lvl2_test_imputer.shape)))\n",
        "print(\"After applying time series feature (24 hours), train, dev, and test statistic: {}, {}, {}\".format((lvl2_train_imputer.shape[0] / 24), (lvl2_dev_imputer.shape[0] / 24), (lvl2_test_imputer.shape[0] / 24)))\n",
        "\n",
        "patients_ids = []\n",
        "for entry in Ys.index:\n",
        "  patients_ids.append(entry[0])\n",
        "\n",
        "print(\"Number of total patient {}\".format(len(patients_ids)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMUZbhsJOmxJ",
        "outputId": "098ba09c-0db2-4978-af29-92698a094423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train, dev, test (402240, 312), (57456, 312), (114960, 312).\n",
            "After applying time series feature (24 hours), train, dev, and test statistic: 16760.0, 2394.0, 4790.0\n",
            "Number of total patient 23944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extracting Time-Series Features and Preprocessing Clinical Notes\n",
        "\n",
        "This step was run separately and the ouput \"preprocessed_notes.p\" was uploaded to the data folder. The code is provided in \"03-Preprocessing-Clinical-Notes\"\n"
      ],
      "metadata": {
        "id": "NfYdP_s5t9mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract Medical Entities in Clinical Notes\n",
        "This step was executed locally on a PC, and the resulting output file \"ner_df.p\" was uploaded to the data folder. The code for this process is provided in the notebook titled \"04 - Apply-med7-on-Clinical-Notes.ipynb.\"\n",
        "\n",
        "Given that \"preprocessed_notes\" comprises over 200,000 rows of data, we opted to extract medical entities solely from the first 5000 entries for the draft submission. However, for the final submission, we aim to expand the extraction process to include more entries."
      ],
      "metadata": {
        "id": "ASZEJ41WvA7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Represent Entities with Different Embeddings\n",
        "\n",
        "This step was run locally on a pc and the output \"new_ner_word2vec_dict.pkl\" was uploaded to the data folder. The code is provided in \"05_Represent_Entities_With_Different_Embeddings.ipynb.\"\n",
        "\n",
        "\n",
        "We have currently integrated Word2Vec embeddings for training the timeseries baseline model. Our next steps involve incorporating FastText and combined embeddings for both the multimodal baseline model and the proposed model."
      ],
      "metadata": {
        "id": "nQs58NoJQHnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Timeseries Data\n",
        "\n",
        "This step was run locally on a pc and the output was uploaded to the data folder. The code is provided in \"06_Create_Timeseries_Data.ipynb.\""
      ],
      "metadata": {
        "id": "izOriQcNvkdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "We have successfully implemented the timeseries baseline model, which runs and prints performance metrics such as AUC, AUPRC, accuracy (ACC), and F1 score. To demonstrate implementation progress, we have temporarily reduced the epoch number from 100 as stated in the original paper to 3 in this notebook.\n",
        "\n",
        "Our next steps involve completing the implementation of the Multimodal baseline model and the proposed model."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Timeseries Baseline\n",
        "\n",
        "We use Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU) and compare their AUC and AUPRC performances in capturing temporal information between patient features."
      ],
      "metadata": {
        "id": "fTmQ0Cpklsyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset Keras Session\n",
        "def reset_keras(model):\n",
        "    sess = get_session()\n",
        "    clear_session()\n",
        "    sess.close()\n",
        "    sess = get_session()\n",
        "\n",
        "    try:\n",
        "        del model # this is from global space - change this as you need\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    gc.collect() # if it's done something you should see a number being outputted\n",
        "\n",
        "def make_prediction_timeseries(model, test_data):\n",
        "    probs = model.predict(test_data)\n",
        "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
        "    return probs, y_pred\n",
        "\n",
        "def save_scores_timeseries(predictions, probs, ground_truth, model_name,\n",
        "                problem_type, iteration, hidden_unit_size, type_of_ner):\n",
        "\n",
        "    auc = roc_auc_score(ground_truth, probs)\n",
        "    auprc = average_precision_score(ground_truth, probs)\n",
        "    acc   = accuracy_score(ground_truth, predictions)\n",
        "    F1    = f1_score(ground_truth, predictions)\n",
        "\n",
        "\n",
        "    result_dict = {}\n",
        "    result_dict['auc'] = auc\n",
        "    result_dict['auprc'] = auprc\n",
        "    result_dict['acc'] = acc\n",
        "    result_dict['F1'] = F1\n",
        "\n",
        "\n",
        "    file_name = str(hidden_unit_size)+\"-\"+model_name+\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\".p\"\n",
        "\n",
        "    result_path = \"/content/drive/MyDrive/CS598_Project/results/\"\n",
        "    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
        "\n",
        "    print(\"AUC: {}, AUPRC: {}, Accuracy: {}, F1 Score: {}\".format(auc, auprc, acc, F1))\n",
        "\n",
        "#def l2_regularizer(scale):\n",
        "#    return tf.keras.regularizers.l2(scale)\n",
        "\n",
        "def timeseries_model(layer_name, number_of_unit):\n",
        "    K.clear_session()\n",
        "\n",
        "    sequence_input = Input(shape=(24,104),  name = \"timeseries_input\")\n",
        "\n",
        "    if layer_name == \"LSTM\":\n",
        "        x = LSTM(number_of_unit)(sequence_input)\n",
        "    else:\n",
        "        x = GRU(number_of_unit)(sequence_input)\n",
        "\n",
        "    #logits_regularizer = tf.keras.regularizers.l2(0.01)\n",
        "    logits_regularizer = keras.regularizers.l2(0.01)\n",
        "    sigmoid_pred = Dense(1, activation='sigmoid',use_bias=False,\n",
        "                         kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
        "                  kernel_regularizer=logits_regularizer)(x)\n",
        "\n",
        "\n",
        "    model = Model(inputs=sequence_input, outputs=sigmoid_pred)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "type_of_ner = \"new\"\n",
        "\n",
        "x_train_lstm = pd.read_pickle(os.path.join(DATAPATH, type_of_ner+\"_x_train.pkl\"))\n",
        "x_dev_lstm = pd.read_pickle(os.path.join(DATAPATH, type_of_ner+\"_x_dev.pkl\"))\n",
        "x_test_lstm = pd.read_pickle(os.path.join(DATAPATH, type_of_ner+\"_x_test.pkl\"))\n",
        "\n",
        "y_train = pd.read_pickle(os.path.join(DATAPATH, type_of_ner+\"_y_train.pkl\"))\n",
        "y_dev = pd.read_pickle(os.path.join(DATAPATH, type_of_ner+\"_y_dev.pkl\"))\n",
        "y_test = pd.read_pickle(os.path.join(DATAPATH, type_of_ner+\"_y_test.pkl\"))\n",
        "\n",
        "epoch_num = 3\n",
        "model_patience = 3\n",
        "monitor_criteria = 'val_loss'\n",
        "batch_size = 128\n",
        "\n",
        "unit_sizes = [128, 256]\n",
        "#unit_sizes = [256]\n",
        "iter_num = 11\n",
        "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
        "layers = [\"GRU\"]\n",
        "#layers = [\"LSTM\", \"GRU\"]\n",
        "for each_layer in layers:\n",
        "    print(\"Layer: \", each_layer)\n",
        "    for each_unit_size in unit_sizes:\n",
        "        print(\"Hidden unit: \", each_unit_size)\n",
        "        for iteration in range(1, iter_num):\n",
        "            print(\"Iteration number: \", iteration)\n",
        "            print(\"=============================\")\n",
        "\n",
        "            for each_problem in target_problems:\n",
        "                print (\"Problem type: \", each_problem)\n",
        "                print (\"__________________\")\n",
        "\n",
        "\n",
        "                early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n",
        "                best_model_name = str(each_layer)+\"-\"+str(each_unit_size)+\"-\"+str(each_problem)+\"-\"+\"best_model.keras\"\n",
        "                checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,\n",
        "                    save_best_only=True, mode='min')\n",
        "\n",
        "\n",
        "                callbacks = [early_stopping_monitor, checkpoint]\n",
        "\n",
        "                model = timeseries_model(each_layer, each_unit_size)\n",
        "                model.fit(x_train_lstm, y_train[each_problem], epochs=epoch_num, verbose=1,\n",
        "                          validation_data=(x_dev_lstm, y_dev[each_problem]), callbacks=callbacks, batch_size= batch_size)\n",
        "\n",
        "                model.load_weights(best_model_name)\n",
        "\n",
        "                probs, predictions = make_prediction_timeseries(model, x_test_lstm)\n",
        "                save_scores_timeseries(predictions, probs, y_test[each_problem].values,str(each_layer),\n",
        "                                       each_problem, iteration, each_unit_size,type_of_ner)\n",
        "                reset_keras(model)\n",
        "                #del model\n",
        "                clear_session()\n",
        "                gc.collect()\n"
      ],
      "metadata": {
        "id": "vhY5Fx2DlPWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd847b1-ed1a-408b-fed6-21077b2fdb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer:  GRU\n",
            "Hidden unit:  128\n",
            "Iteration number:  1\n",
            "=============================\n",
            "Problem type:  mort_hosp\n",
            "__________________\n",
            "Epoch 1/3\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.6795 - acc: 0.6484\n",
            "Epoch 1: val_loss improved from inf to 0.62753, saving model to GRU-128-mort_hosp-best_model.keras\n",
            "2/2 [==============================] - 4s 749ms/step - loss: 0.6665 - acc: 0.6467 - val_loss: 0.6275 - val_acc: 0.7179\n",
            "Epoch 2/3\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5797 - acc: 0.7422\n",
            "Epoch 2: val_loss improved from 0.62753 to 0.61716, saving model to GRU-128-mort_hosp-best_model.keras\n",
            "2/2 [==============================] - 0s 118ms/step - loss: 0.5678 - acc: 0.7500 - val_loss: 0.6172 - val_acc: 0.6410\n",
            "Epoch 3/3\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5246 - acc: 0.7734\n",
            "Epoch 3: val_loss improved from 0.61716 to 0.61372, saving model to GRU-128-mort_hosp-best_model.keras\n",
            "2/2 [==============================] - 0s 128ms/step - loss: 0.4976 - acc: 0.8043 - val_loss: 0.6137 - val_acc: 0.6410\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "AUC: 0.7659574468085106, AUPRC: 0.39722222222222214, Accuracy: 0.8269230769230769, F1 Score: 0.47058823529411764\n",
            "Problem type:  mort_icu\n",
            "__________________\n",
            "Epoch 1/3\n",
            "1/2 [==============>...............] - ETA: 2s - loss: 0.7419 - acc: 0.5000\n",
            "Epoch 1: val_loss improved from inf to 0.69285, saving model to GRU-128-mort_icu-best_model.keras\n",
            "2/2 [==============================] - 3s 942ms/step - loss: 0.7171 - acc: 0.5380 - val_loss: 0.6928 - val_acc: 0.5385\n",
            "Epoch 2/3\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5977 - acc: 0.7120\n",
            "Epoch 2: val_loss improved from 0.69285 to 0.64099, saving model to GRU-128-mort_icu-best_model.keras\n",
            "2/2 [==============================] - 0s 193ms/step - loss: 0.5977 - acc: 0.7120 - val_loss: 0.6410 - val_acc: 0.6667\n",
            "Epoch 3/3\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5180 - acc: 0.8043\n",
            "Epoch 3: val_loss improved from 0.64099 to 0.60640, saving model to GRU-128-mort_icu-best_model.keras\n",
            "2/2 [==============================] - 0s 242ms/step - loss: 0.5180 - acc: 0.8043 - val_loss: 0.6064 - val_acc: 0.7179\n",
            "2/2 [==============================] - 1s 23ms/step\n",
            "AUC: 0.5850340136054422, AUPRC: 0.15136054421768708, Accuracy: 0.7307692307692307, F1 Score: 0.2222222222222222\n",
            "Problem type:  los_3\n",
            "__________________\n",
            "Epoch 1/3\n",
            "1/2 [==============>...............] - ETA: 2s - loss: 0.7573 - acc: 0.4766\n",
            "Epoch 1: val_loss improved from inf to 0.59359, saving model to GRU-128-los_3-best_model.keras\n",
            "2/2 [==============================] - 3s 651ms/step - loss: 0.7409 - acc: 0.5054 - val_loss: 0.5936 - val_acc: 0.7692\n",
            "Epoch 2/3\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.6958 - acc: 0.5703\n",
            "Epoch 2: val_loss improved from 0.59359 to 0.59005, saving model to GRU-128-los_3-best_model.keras\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 0.6827 - acc: 0.5978 - val_loss: 0.5900 - val_acc: 0.7692\n",
            "Epoch 3/3\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6423 - acc: 0.6630\n",
            "Epoch 3: val_loss improved from 0.59005 to 0.58412, saving model to GRU-128-los_3-best_model.keras\n",
            "2/2 [==============================] - 0s 156ms/step - loss: 0.6423 - acc: 0.6630 - val_loss: 0.5841 - val_acc: 0.6923\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "AUC: 0.69375, AUPRC: 0.6008614413637534, Accuracy: 0.7115384615384616, F1 Score: 0.5945945945945946\n",
            "Problem type:  los_7\n",
            "__________________\n",
            "Epoch 1/3\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6763 - acc: 0.6304"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function Model.make_test_function.<locals>.test_function at 0x7d960dc300d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.73184, saving model to GRU-128-los_7-best_model.keras\n",
            "2/2 [==============================] - 3s 689ms/step - loss: 0.6763 - acc: 0.6304 - val_loss: 0.7318 - val_acc: 0.4615\n",
            "Epoch 2/3\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5813 - acc: 0.8047\n",
            "Epoch 2: val_loss improved from 0.73184 to 0.69803, saving model to GRU-128-los_7-best_model.keras\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 0.5649 - acc: 0.8152 - val_loss: 0.6980 - val_acc: 0.5897\n",
            "Epoch 3/3\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.4945 - acc: 0.8828\n",
            "Epoch 3: val_loss improved from 0.69803 to 0.67656, saving model to GRU-128-los_7-best_model.keras\n",
            "2/2 [==============================] - 0s 142ms/step - loss: 0.4791 - acc: 0.8804 - val_loss: 0.6766 - val_acc: 0.6154\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "AUC: 0.46808510638297873, AUPRC: 0.12302621867077436, Accuracy: 0.7307692307692307, F1 Score: 0.2222222222222222\n",
            "Iteration number:  2\n",
            "=============================\n",
            "Problem type:  mort_hosp\n",
            "__________________\n",
            "Epoch 1/3\n",
            "1/2 [==============>...............] - ETA: 2s - loss: 0.7472 - acc: 0.5234"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7d960cb8b370> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.68054, saving model to GRU-128-mort_hosp-best_model.keras\n",
            "2/2 [==============================] - 3s 659ms/step - loss: 0.7392 - acc: 0.5543 - val_loss: 0.6805 - val_acc: 0.5641\n",
            "Epoch 2/3\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6202 - acc: 0.7500\n",
            "Epoch 2: val_loss improved from 0.68054 to 0.64475, saving model to GRU-128-mort_hosp-best_model.keras\n",
            "2/2 [==============================] - 0s 151ms/step - loss: 0.6202 - acc: 0.7500 - val_loss: 0.6447 - val_acc: 0.5641\n",
            "Epoch 3/3\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5406 - acc: 0.7772\n",
            "Epoch 3: val_loss improved from 0.64475 to 0.62373, saving model to GRU-128-mort_hosp-best_model.keras\n",
            "2/2 [==============================] - 0s 149ms/step - loss: 0.5406 - acc: 0.7772 - val_loss: 0.6237 - val_acc: 0.5897\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "AUC: 0.7404255319148937, AUPRC: 0.3565891472868217, Accuracy: 0.75, F1 Score: 0.3157894736842105\n",
            "Problem type:  mort_icu\n",
            "__________________\n",
            "Epoch 1/3\n",
            "1/2 [==============>...............] - ETA: 3s - loss: 0.7495 - acc: 0.4766\n",
            "Epoch 1: val_loss improved from inf to 0.62940, saving model to GRU-128-mort_icu-best_model.keras\n",
            "2/2 [==============================] - 4s 682ms/step - loss: 0.7410 - acc: 0.5163 - val_loss: 0.6294 - val_acc: 0.7436\n",
            "Epoch 2/3\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.6496 - acc: 0.6953\n",
            "Epoch 2: val_loss improved from 0.62940 to 0.59330, saving model to GRU-128-mort_icu-best_model.keras\n",
            "2/2 [==============================] - 0s 124ms/step - loss: 0.6272 - acc: 0.7283 - val_loss: 0.5933 - val_acc: 0.7179\n",
            "Epoch 3/3\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5324 - acc: 0.8516\n",
            "Epoch 3: val_loss improved from 0.59330 to 0.56961, saving model to GRU-128-mort_icu-best_model.keras\n",
            "2/2 [==============================] - 0s 127ms/step - loss: 0.5427 - acc: 0.8207 - val_loss: 0.5696 - val_acc: 0.7179\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "AUC: 0.6394557823129251, AUPRC: 0.4152159896840748, Accuracy: 0.7307692307692307, F1 Score: 0.2222222222222222\n",
            "Problem type:  los_3\n",
            "__________________\n",
            "Epoch 1/3\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6938 - acc: 0.6087\n",
            "Epoch 1: val_loss improved from inf to 0.68592, saving model to GRU-128-los_3-best_model.keras\n",
            "2/2 [==============================] - 4s 1s/step - loss: 0.6938 - acc: 0.6087 - val_loss: 0.6859 - val_acc: 0.5897\n",
            "Epoch 2/3\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6425 - acc: 0.6848\n",
            "Epoch 2: val_loss improved from 0.68592 to 0.67219, saving model to GRU-128-los_3-best_model.keras\n",
            "2/2 [==============================] - 0s 231ms/step - loss: 0.6425 - acc: 0.6848 - val_loss: 0.6722 - val_acc: 0.6154\n",
            "Epoch 3/3\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6062 - acc: 0.7228\n",
            "Epoch 3: val_loss improved from 0.67219 to 0.66323, saving model to GRU-128-los_3-best_model.keras\n",
            "2/2 [==============================] - 0s 207ms/step - loss: 0.6062 - acc: 0.7228 - val_loss: 0.6632 - val_acc: 0.5641\n",
            "2/2 [==============================] - 1s 16ms/step\n",
            "AUC: 0.6546875, AUPRC: 0.6364966691410696, Accuracy: 0.6346153846153846, F1 Score: 0.48648648648648646\n",
            "Problem type:  los_7\n",
            "__________________\n",
            "Epoch 1/3\n",
            "1/2 [==============>...............] - ETA: 2s - loss: 0.7141 - acc: 0.5547\n",
            "Epoch 1: val_loss improved from inf to 0.66920, saving model to GRU-128-los_7-best_model.keras\n",
            "2/2 [==============================] - 3s 689ms/step - loss: 0.6940 - acc: 0.5815 - val_loss: 0.6692 - val_acc: 0.5897\n",
            "Epoch 2/3\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5876 - acc: 0.7500\n",
            "Epoch 2: val_loss improved from 0.66920 to 0.66211, saving model to GRU-128-los_7-best_model.keras\n",
            "2/2 [==============================] - 0s 123ms/step - loss: 0.5799 - acc: 0.7500 - val_loss: 0.6621 - val_acc: 0.6154\n",
            "Epoch 3/3\n",
            "1/2 [==============>...............] - ETA: 0s - loss: 0.5227 - acc: 0.7891\n",
            "Epoch 3: val_loss improved from 0.66211 to 0.66152, saving model to GRU-128-los_7-best_model.keras\n",
            "2/2 [==============================] - 0s 121ms/step - loss: 0.4880 - acc: 0.8424 - val_loss: 0.6615 - val_acc: 0.6410\n",
            "2/2 [==============================] - 1s 16ms/step\n",
            "AUC: 0.651063829787234, AUPRC: 0.17731148162182644, Accuracy: 0.7692307692307693, F1 Score: 0.33333333333333337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Multimodal Baseline\n",
        "\n",
        "We use the pre-trained NER model, med7, to extract different named medical entities and represent them with two different embedding methods - word embedding and document embedding."
      ],
      "metadata": {
        "id": "xE2Y_Hy4mCcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalModelTrainer:\n",
        "    def __init__(self, type_of_ner):\n",
        "        self.type_of_ner = type_of_ner\n",
        "\n",
        "    def reset_keras(self, model):\n",
        "        clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "    def create_dataset(self, dict_of_ner):\n",
        "        temp_data = []\n",
        "        for k, v in sorted(dict_of_ner.items()):\n",
        "            temp = []\n",
        "            for embed in v:\n",
        "                temp.append(embed)\n",
        "            temp_data.append(np.mean(temp, axis=0))\n",
        "        return np.asarray(temp_data)\n",
        "\n",
        "    def make_prediction_multi_avg(self, model, test_data):\n",
        "        probs = model.predict(test_data)\n",
        "        y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
        "        return probs, y_pred\n",
        "\n",
        "    def save_scores_multi_avg(self, predictions, probs, ground_truth,\n",
        "                              embed_name, problem_type, iteration, hidden_unit_size,\n",
        "                              sequence_name):\n",
        "        auc = roc_auc_score(ground_truth, probs)\n",
        "        auprc = average_precision_score(ground_truth, probs)\n",
        "        acc   = accuracy_score(ground_truth, predictions)\n",
        "        F1    = f1_score(ground_truth, predictions)\n",
        "\n",
        "        result_dict = {}\n",
        "        result_dict['auc'] = auc\n",
        "        result_dict['auprc'] = auprc\n",
        "        result_dict['acc'] = acc\n",
        "        result_dict['F1'] = F1\n",
        "\n",
        "        result_path = \"results/\"\n",
        "        file_name = f\"{sequence_name}-{hidden_unit_size}-{embed_name}-{problem_type}-{iteration}-{self.type_of_ner}-avg-.p\"\n",
        "        pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
        "\n",
        "        print(auc, auprc, acc, F1)\n",
        "\n",
        "    def avg_ner_model(self, layer_name, number_of_unit, embedding_name):\n",
        "        if embedding_name == \"concat\":\n",
        "            input_dimension = 200\n",
        "        else:\n",
        "            input_dimension = 100\n",
        "\n",
        "        sequence_input = Input(shape=(24,104))\n",
        "        input_avg = Input(shape=(input_dimension, ), name=\"avg\")\n",
        "\n",
        "        if layer_name == \"GRU\":\n",
        "            x = GRU(number_of_unit)(sequence_input)\n",
        "        elif layer_name == \"LSTM\":\n",
        "            x = LSTM(number_of_unit)(sequence_input)\n",
        "\n",
        "        x = Concatenate()([x, input_avg])\n",
        "        x = Dense(256, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "\n",
        "        logits_regularizer = tf.keras.regularizers.l2(0.01)\n",
        "        preds = Dense(1, activation='sigmoid', use_bias=False,\n",
        "                      kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
        "                      kernel_regularizer=logits_regularizer)(x)\n",
        "\n",
        "        opt = Adam(lr=0.001, decay=0.01)\n",
        "        model = Model(inputs=[sequence_input, input_avg], outputs=preds)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                      optimizer=opt,\n",
        "                      metrics=['acc'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_models(self):\n",
        "        x_train_lstm = pd.read_pickle(f\"data/{self.type_of_ner}_x_train.pkl\")\n",
        "        x_dev_lstm = pd.read_pickle(f\"data/{self.type_of_ner}_x_dev.pkl\")\n",
        "        x_test_lstm = pd.read_pickle(f\"data/{self.type_of_ner}_x_test.pkl\")\n",
        "\n",
        "        y_train = pd.read_pickle(f\"data/{self.type_of_ner}_y_train.pkl\")\n",
        "        y_dev = pd.read_pickle(f\"data/{self.type_of_ner}_y_dev.pkl\")\n",
        "        y_test = pd.read_pickle(f\"data/{self.type_of_ner}_y_test.pkl\")\n",
        "\n",
        "        ner_word2vec = pd.read_pickle(f\"data/{self.type_of_ner}_ner_word2vec_limited_dict.pkl\")\n",
        "        ner_fasttext = pd.read_pickle(f\"data/{self.type_of_ner}_ner_fasttext_limited_dict.pkl\")\n",
        "        ner_concat = pd.read_pickle(f\"data/{self.type_of_ner}_ner_combined_limited_dict.pkl\")\n",
        "\n",
        "        train_ids = pd.read_pickle(f\"data/{self.type_of_ner}_train_ids.pkl\")\n",
        "        dev_ids = pd.read_pickle(f\"data/{self.type_of_ner}_dev_ids.pkl\")\n",
        "        test_ids = pd.read_pickle(f\"data/{self.type_of_ner}_test_ids.pkl\")\n",
        "\n",
        "        embedding_types = ['word2vec', 'fasttext', 'concat']\n",
        "        embedding_dict = [ner_word2vec, ner_fasttext, ner_concat]\n",
        "        target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
        "\n",
        "        num_epoch = 100\n",
        "        model_patience = 5\n",
        "        monitor_criteria = 'val_loss'\n",
        "        batch_size = 64\n",
        "        iter_num = 2\n",
        "        unit_sizes = [128, 256]\n",
        "\n",
        "        layers = [\"GRU\"]\n",
        "        for each_layer in layers:\n",
        "            print (\"Layer: \", each_layer)\n",
        "            for each_unit_size in unit_sizes:\n",
        "                print (\"Hidden unit: \", each_unit_size)\n",
        "\n",
        "                for embed_dict, embed_name in zip(embedding_dict, embedding_types):\n",
        "                    print (\"Embedding: \", embed_name)\n",
        "                    print(\"=============================\")\n",
        "\n",
        "                    temp_train_ner = dict((k, ner_word2vec[k]) for k in train_ids)\n",
        "                    temp_dev_ner = dict((k, ner_word2vec[k]) for k in dev_ids)\n",
        "                    temp_test_ner = dict((k, ner_word2vec[k]) for k in test_ids)\n",
        "\n",
        "                    x_train_ner = self.create_dataset(temp_train_ner)\n",
        "                    x_dev_ner = self.create_dataset(temp_dev_ner)\n",
        "                    x_test_ner = self.create_dataset(temp_test_ner)\n",
        "\n",
        "                    for iteration in range(1, iter_num):\n",
        "                        print (\"Iteration number: \", iteration)\n",
        "\n",
        "                        for each_problem in target_problems:\n",
        "                            print (\"Problem type: \", each_problem)\n",
        "                            print (\"__________________\")\n",
        "\n",
        "                            early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n",
        "                            best_model_name = f\"avg-{embed_name}-{each_problem}-best_model.hdf5\"\n",
        "                            checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,\n",
        "                                                         save_best_only=True, mode='min', period=1)\n",
        "\n",
        "                            callbacks = [early_stopping_monitor, checkpoint]\n",
        "\n",
        "                            model = self.avg_ner_model(each_layer, each_unit_size, embed_name)\n",
        "\n",
        "                            model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1,\n",
        "                                      validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks,\n",
        "                                      batch_size=batch_size )\n",
        "\n",
        "                            model.load_weights(best_model_name)\n",
        "\n",
        "                            probs, predictions = self.make_prediction_multi_avg(model, [x_test_lstm, x_test_ner])\n",
        "\n",
        "                            self.save_scores_multi_avg(predictions, probs, y_test[each_problem], embed_name, each_problem,\n",
        "                                                        iteration, each_unit_size, each_layer)\n",
        "\n",
        "                            self.reset_keras(model)"
      ],
      "metadata": {
        "id": "Mok8dEygmJ5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Proposed Model\n",
        "\n",
        "We leverage 1D Convolutional Neural Networks to extract features from medical entities, subsequently integrating them with recurrent and fully-connected layers for comprehensive patient representation"
      ],
      "metadata": {
        "id": "sU4URWIvm71m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProposedModel:\n",
        "    def __init__(self, type_of_ner):\n",
        "        self.type_of_ner = type_of_ner\n",
        "\n",
        "    def make_prediction_cnn(self, model, test_data):\n",
        "        probs = model.predict(test_data)\n",
        "        y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
        "        return probs, y_pred\n",
        "\n",
        "    def save_scores_cnn(self, predictions, probs, ground_truth,\n",
        "                        embed_name, problem_type, iteration, hidden_unit_size,\n",
        "                        sequence_name):\n",
        "        auc = roc_auc_score(ground_truth, probs)\n",
        "        auprc = average_precision_score(ground_truth, probs)\n",
        "        acc   = accuracy_score(ground_truth, predictions)\n",
        "        F1    = f1_score(ground_truth, predictions)\n",
        "\n",
        "        result_dict = {}\n",
        "        result_dict['auc'] = auc\n",
        "        result_dict['auprc'] = auprc\n",
        "        result_dict['acc'] = acc\n",
        "        result_dict['F1'] = F1\n",
        "\n",
        "        result_path = \"results/cnn/\"\n",
        "        file_name = f\"{sequence_name}-{hidden_unit_size}-{embed_name}-{problem_type}-{iteration}-{self.type_of_ner}-cnn-.p\"\n",
        "        pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
        "\n",
        "        print(auc, auprc, acc, F1)\n",
        "\n",
        "    def proposedmodel(self, layer_name, number_of_unit, embedding_name, ner_limit, num_filter):\n",
        "        if embedding_name == \"concat\":\n",
        "            input_dimension = 200\n",
        "        else:\n",
        "            input_dimension = 100\n",
        "\n",
        "        sequence_input = Input(shape=(24,104))\n",
        "        input_img = Input(shape=(ner_limit, input_dimension), name=\"cnn_input\")\n",
        "\n",
        "        text_conv1d = Conv1D(filters=num_filter, kernel_size=3,\n",
        "                             padding='valid', strides=1, dilation_rate=1, activation='relu',\n",
        "                             kernel_initializer=tf.contrib.layers.xavier_initializer())(input_img)\n",
        "\n",
        "        text_conv1d = Conv1D(filters=num_filter*2, kernel_size=3,\n",
        "                             padding='valid', strides=1, dilation_rate=1, activation='relu',\n",
        "                             kernel_initializer=tf.contrib.layers.xavier_initializer())(text_conv1d)\n",
        "\n",
        "        text_conv1d = Conv1D(filters=num_filter*3, kernel_size=3,\n",
        "                             padding='valid', strides=1, dilation_rate=1, activation='relu',\n",
        "                             kernel_initializer=tf.contrib.layers.xavier_initializer())(text_conv1d)\n",
        "\n",
        "        text_embeddings = GlobalMaxPooling1D()(text_conv1d)\n",
        "\n",
        "        if layer_name == \"GRU\":\n",
        "            x = GRU(number_of_unit)(sequence_input)\n",
        "        elif layer_name == \"LSTM\":\n",
        "            x = LSTM(number_of_unit)(sequence_input)\n",
        "\n",
        "        concatenated = Concatenate()([x, text_embeddings])\n",
        "        concatenated = Dense(512, activation='relu')(concatenated)\n",
        "        concatenated = Dropout(0.2)(concatenated)\n",
        "\n",
        "        logits_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
        "        preds = Dense(1, activation='sigmoid', use_bias=False,\n",
        "                             kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "                             kernel_regularizer=logits_regularizer)(concatenated)\n",
        "\n",
        "        opt = Adam(lr=1e-3, decay=0.01)\n",
        "        model = Model(inputs=[sequence_input, input_img], outputs=preds)\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                      optimizer=opt,\n",
        "                      metrics=['acc'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_models(self):\n",
        "        # Loading data and setting up parameters\n",
        "        # (x_train_lstm, x_dev_lstm, x_test_lstm, y_train, y_dev, y_test, ner_word2vec, ner_fasttext,\n",
        "        #  ner_concat, train_ids, dev_ids, test_ids) should be defined before calling this method\n",
        "\n",
        "        embedding_types = ['word2vec', 'fasttext', 'concat']\n",
        "        embedding_dict = [ner_word2vec, ner_fasttext, ner_concat]\n",
        "        target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
        "\n",
        "        num_epoch = 100\n",
        "        model_patience = 5\n",
        "        monitor_criteria = 'val_loss'\n",
        "        batch_size = 64\n",
        "        filter_number = 32\n",
        "        ner_representation_limit = 64\n",
        "\n",
        "        sequence_model = \"GRU\"\n",
        "        sequence_hidden_unit = 256\n",
        "\n",
        "        maxiter = 11\n",
        "\n",
        "        for embed_dict, embed_name in zip(embedding_dict, embedding_types):\n",
        "            print (\"Embedding: \", embed_name)\n",
        "            print(\"=============================\")\n",
        "\n",
        "            temp_train_ner = dict((k, embed_dict[k]) for k in train_ids)\n",
        "            tem_dev_ner = dict((k, embed_dict[k]) for k in dev_ids)\n",
        "            temp_test_ner = dict((k, embed_dict[k]) for k in test_ids)\n",
        "\n",
        "            x_train_dict = {}\n",
        "            x_dev_dict = {}\n",
        "            x_test_dict = {}\n",
        "\n",
        "            x_train_dict = self.get_subvector_data(ner_representation_limit, embed_name, temp_train_ner)\n",
        "            x_dev_dict = self.get_subvector_data(ner_representation_limit, embed_name, tem_dev_ner)\n",
        "            x_test_dict = self.get_subvector_data(ner_representation_limit, embed_name, temp_test_ner)\n",
        "\n",
        "            x_train_dict_sorted = collections.OrderedDict(sorted(x_train_dict.items()))\n",
        "            x_dev_dict_sorted = collections.OrderedDict(sorted(x_dev_dict.items()))\n",
        "            x_test_dict_sorted = collections.OrderedDict(sorted(x_test_dict.items()))\n",
        "\n",
        "            x_train_ner = np.asarray(x_train_dict_sorted.values())\n",
        "            x_dev_ner = np.asarray(x_dev_dict_sorted.values())\n",
        "            x_test_ner = np.asarray(x_test_dict_sorted.values())\n",
        "\n",
        "            for iteration in range(1,maxiter):\n",
        "                print (\"Iteration number: \", iteration)\n",
        "\n",
        "                for each_problem in target_problems:\n",
        "                    print (\"Problem type: \", each_problem)\n",
        "                    print (\"__________________\")\n",
        "\n",
        "                    early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n",
        "                    best_model_name = f\"{ner_representation_limit}-basiccnn1d-{embed_name}-{each_problem}-best_model.hdf5\"\n",
        "                    checkpoint = ModelCheckpoint(best_model_name, monitor=monitor_criteria, verbose=1,\n",
        "                                                 save_best_only=True, mode='min')\n",
        "\n",
        "                    reduce_lr = ReduceLROnPlateau(monitor=monitor_criteria, factor=0.2,\n",
        "                                      patience=2, min_lr=0.00001, epsilon=1e-4, mode='min')\n",
        "\n",
        "                    callbacks = [early_stopping_monitor, checkpoint, reduce_lr]\n",
        "\n",
        "                    model = self.proposedmodel(sequence_model, sequence_hidden_unit,\n",
        "                                               embed_name, ner_representation_limit, filter_number)\n",
        "                    model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1,\n",
        "                              validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, batch_size=batch_size)\n",
        "\n",
        "                    probs, predictions = self.make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n",
        "                    self.print_scores_cnn(predictions, probs, y_test[each_problem], embed_name, each_problem, iteration, sequence_hidden_unit)\n",
        "\n",
        "                    model.load_weights(best_model_name)\n",
        "\n",
        "                    probs, predictions = self.make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n",
        "                    self.save_scores_cnn(predictions, probs, y_test[each_problem], embed_name, each_problem, iteration,\n",
        "                                        sequence_hidden_unit, sequence_model)\n",
        "                    del model\n",
        "                    clear_session()\n",
        "                    gc.collect()"
      ],
      "metadata": {
        "id": "moPFH7-Cd31h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "The paper suggests the computational requirements are:  \n",
        "  NVIDIA Tesla K80 GPU with 24 GB of VRAM, 378 GB of RAM and Intel Xeon E5 2683 processor.\n",
        "\n",
        "Because the MIMIC-III dataset is so large, we ran the data preprocessing part locally on a CPU computer with 16 GB of VRAM, 32GB of RAM and Intel i9-12900H processor.\n",
        "\n",
        "The training results are saved in the results folder."
      ],
      "metadata": {
        "id": "Yv5NCgolTDz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "We will be using 3 different statistical methods for the comparison of our models.\n",
        "* Area Under the Receiver Operating Characteristic curve (AUROC), which is the area under the true positive rate versus the false positive rate.\n",
        "* Area Under the Precision-Recall Curve (AUPRC), which is the area under\n",
        "the precision versus recall plot.\n",
        "* F1 score measures accuracy by considering both precision and recall to compute the score, providing a balance between false positives and false negatives."
      ],
      "metadata": {
        "id": "kMoKaBruTInf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show evaluation results\n",
        "\n",
        "#Results from Timeseries Baseline Model\n",
        "# List all files in the results directory\n",
        "result_path = \"/content/drive/MyDrive/CS598_Project/results/\"\n",
        "result_files = os.listdir(result_path)\n",
        "\n",
        "results_dfs = []\n",
        "\n",
        "# Loop through each result file\n",
        "for file_name in result_files:\n",
        "\n",
        "    # Load the result dictionary from the file\n",
        "    result_dict = pd.read_pickle(os.path.join(result_path, file_name))\n",
        "\n",
        "    # Extract relevant information from the file name\n",
        "    parts = file_name.split(\"-\")\n",
        "    model = parts[0]\n",
        "    problem_type = parts[2]\n",
        "    iteration = int(parts[3])\n",
        "\n",
        "    # Extract performance metrics from the result dictionary\n",
        "    auc = result_dict[\"auc\"]\n",
        "    auprc = result_dict[\"auprc\"]\n",
        "    accuracy = result_dict[\"acc\"]\n",
        "    f1 = result_dict[\"F1\"]\n",
        "\n",
        "    # Append the results to the DataFrame\n",
        "    result_df = pd.DataFrame({\n",
        "        \"Model\": [model],\n",
        "        \"Problem Type\": [problem_type],\n",
        "        \"Iteration\": [iteration],\n",
        "        \"AUC\": [auc],\n",
        "        \"AUPRC\": [auprc],\n",
        "        \"Accuracy\": [accuracy],\n",
        "        \"F1\": [f1]\n",
        "    })\n",
        "\n",
        "    results_dfs.append(result_df)\n",
        "\n",
        "results_df = pd.concat(results_dfs, ignore_index=True)\n",
        "\n",
        "# Display the DataFrame with the results\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "4b0_FSNgLzxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc743a57-317d-46fe-ea47-a56997a75122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Model Problem Type  Iteration       AUC     AUPRC  Accuracy        F1\n",
            "0     128    mort_hosp          1  0.812766  0.297210  0.884615  0.400000\n",
            "1     128     mort_icu          1  0.768707  0.239683  0.903846  0.285714\n",
            "2     128        los_3          1  0.571875  0.442925  0.634615  0.424242\n",
            "3     128        los_7          1  0.497872  0.111536  0.826923  0.000000\n",
            "4     128    mort_hosp          2  0.748936  0.410932  0.846154  0.333333\n",
            "..    ...          ...        ...       ...       ...       ...       ...\n",
            "155   256        los_7          9  0.608511  0.132026  0.634615  0.095238\n",
            "156   256    mort_hosp         10  0.859574  0.566667  0.884615  0.571429\n",
            "157   256     mort_icu         10  0.687075  0.167057  0.846154  0.333333\n",
            "158   256        los_3         10  0.682813  0.581843  0.673077  0.514286\n",
            "159   256        los_7         10  0.412766  0.102592  0.750000  0.133333\n",
            "\n",
            "[160 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "We anticipate our results to be similar to the ones given in the paper. The baseline multimodal model should perform better overall than the baseline GRU model across all 4 clinical task predictions. The proposed model should perform better overall across all 4 clinical task predictions compared to the baseline multimodal model.\n",
        "\n",
        "This section will be updated after we have implemented all our models."
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison\n",
        "\n",
        "We will discuss how our results compared to the results given in the paper. We will complete this section after training the model and gathering the model prediction results for all 4 clinical tasks."
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "The paper is so far reproducible. We will later assess reproducibility of paper when we have finished training the models and compared the model prediction results.\n",
        "\n",
        "What was easy during the reproduction:\n",
        "* all the code was well-documented, which made it easy to find and use\n",
        "\n",
        "What was difficult during the reproduction:\n",
        "* the duration of preprocessing the data was long\n",
        "* retrieving the clinical entities took longer than expected so we had to use a smaller subset of the preprocessing data\n",
        "* downloading large datasets (i.e. MIMIC-III) and gathering the pre-trained models from various sources took a longer time than we thought it would\n",
        "* Fasttext embedding was not available in original repository download link given by the author. The embedding was found in the comments section of the Issues tab in the paper's Github repository. This was very difficult to find.\n",
        "* in the original code, several packages were outdated or deprecated, leading to issues with compatibility and functionality.\n",
        "\n",
        "\n",
        "Suggestions on how to improve the reproducibility:\n",
        "* a note on the duration of each step would have been helpful, i.e. preprocessing data, employing the embedding techniques, training the model, etc.\n",
        "\n",
        "What we would do in the next phase:\n",
        "* can add more feature to improve prediction performance, such as drug ICD codes.\n",
        "* can compare results of using context-dependent word embeddings (i.e. BERT) with context-independent word embeddings (i.e. Word2Vec and FastText) for representing medical entities.\n",
        "\n",
        "\n",
        "## Plans\n",
        "* Due to time constraints, we kept the template code for mounting a google drive. We will change this to using a public link to access the data in the future.\n",
        "* extract medical entities using word embedding techniques on the entire dataset, or a larger subset of the dataset.\n",
        "* represent extracted medical entities\n",
        "* train the models. If training takes a long time, we will train on a small # of epoch (like 3). Alternatively, we will train the model in a different jupyter notebook and just load the pretrained model to this notebook.\n",
        "* Display the figures and metrics resulting from model prediction\n",
        "* Write a function to display model prediction summary of results and figures.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Bardak B, Tan M, \"Improving clinical outcome predictions using convolution over medical entities with multimodal learning\", Artificial Intelligence in Medicine, 2021, 117:0933-3657, doi:https://doi.org/10.1016/j.artmed.2021.102112.\n",
        "2. Johnson A, Pollard T, Mark R, \"MIMIC-III Clinical Database (version 1.4)\", PhysioNet, 2016, doi:https://doi.org/10.13026/C2XW26.\n",
        "3. Johnson AEW, Pollard TJ, Shen L, Lehman LH, Feng M, Ghassemi M, Moody B, Szolovits P, Celi L A, Mark RG, \"MIMIC-III, a freely accessible critical care database\", Scientific Data, 2016, 3:160035.\n",
        "4. Goldberger A, Amaral L, Glass L, Hausdorff J, Ivanov PC, Mark R, ... & Stanley HE, \"PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals\", Circulation [Online], 2000, 101:23, pp. e215â€“e220.\n",
        "5. Choi E, Bahadori MT, Schuetz A, Stewart WF, Sun J. Doctor AI: predicting clinical events via recurrent neural networks. Machine learning for healthcare conference 2016:301-18.\n",
        "6. Choi E, Bahadori MT, Sun J, Kulas J, Schuetz A, Stewart W. Retain: an interpretable predictive model for healthcare using reverse time attention mechanism. Advances in neural information processing systems. 2016. p. 3504-12.\n",
        "7. Caballero Barajas KL, Akella R. Dynamically modeling patientâ€™s health state from electronic medical records: a time series approach. Proceedings of the 21st ACM SIGKDD international conference on knowledge discovery and data mining 2015:69â€“78.\n",
        "8. Song H, Rajan D, Thiagarajan JJ, Spanias A. Attend and diagnose: clinical time series analysis using attention models. Thirty-second AAAI conference on artificial intelligence 2018.\n",
        "9. Suresh H, Gong JJ, Guttag JV. Learning tasks for multitask learning: heterogenous patient populations in the ICU. Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining 2018:802â€“10.\n",
        "10. Lipton ZC, Kale DC, Elkan C, Wetzel R. Learning to diagnose with LSTM recurrent neural networks. 2015 (arXiv preprint), arXiv:1511.03677.\n",
        "11. Wang S, McDermott MBA, Chauhan G, Hughes MC, Naumann T, Ghassemi M. MIMIC-Extract: A Data Extraction, Preprocessing, and Representation\n",
        "Pipeline for MIMIC-III. arXiv:1907.08322.\n",
        "12. Kormilitzin A, Vaci N, Liu Q, Nevado-Holgado A. Med7: A Transferable Clinical Natural Language Processing Model for Electronic Health Records. 2020. arXiv:2003.01271.\n",
        "13. Huang K, Altosaar J, Ranganath R. ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission. 2019. arXiv:1904.05342\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3A5pgB1vtVK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}