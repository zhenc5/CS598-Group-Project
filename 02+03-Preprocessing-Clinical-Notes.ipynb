{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1eSx7cyINuk1"},"outputs":[],"source":["# import libraries\n","import re\n","import os\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk import sent_tokenize, word_tokenize\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AY45P3n4NKnz"},"outputs":[],"source":["## Helper functions for preprocessing text in the data file\n","# This code is taken from https://github.com/kaggarwal/ClinicalNotesICU\n","\n","SECTION_TITLES = re.compile(\n","    r'('\n","    r'ABDOMEN AND PELVIS|CLINICAL HISTORY|CLINICAL INDICATION|COMPARISON|COMPARISON STUDY DATE'\n","    r'|EXAM|EXAMINATION|FINDINGS|HISTORY|IMPRESSION|INDICATION'\n","    r'|MEDICAL CONDITION|PROCEDURE|REASON FOR EXAM|REASON FOR STUDY|REASON FOR THIS EXAMINATION'\n","    r'|TECHNIQUE'\n","    r'):|FINAL REPORT',\n","    re.I | re.M)\n","\n","\n","def getSentences(t):\n","    return list(preprocess_mimic(t))\n","\n","def pattern_repl(matchobj):\n","    \"\"\"\n","    Return a replacement string to be used for match object\n","    \"\"\"\n","    return ' '.rjust(len(matchobj.group(0)))\n","\n","def clean_text(text):\n","    \"\"\"\n","    Clean text\n","    \"\"\"\n","\n","    # Replace [**Patterns**] with spaces.\n","    text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', pattern_repl, text)\n","    # Replace `_` with spaces.\n","    text = re.sub(r'_', ' ', text)\n","\n","    start = 0\n","    end = find_end(text)\n","    new_text = ''\n","    if start > 0:\n","        new_text += ' ' * start\n","    new_text = text[start:end]\n","\n","    # make sure the new text has the same length of old text.\n","    if len(text) - end > 0:\n","        new_text += ' ' * (len(text) - end)\n","    return new_text\n","\n","def preprocess_mimic(text):\n","    \"\"\"\n","    Preprocess reports in MIMIC-III.\n","    1. remove [**Patterns**] and signature\n","    2. split the report into sections\n","    3. tokenize sentences and words\n","    4. lowercase\n","    \"\"\"\n","    for sec in split_heading(clean_text(text)):\n","        for sent in sent_tokenize(sec):\n","            text = ' '.join(word_tokenize(sent))\n","            yield text.lower()\n","\n","def split_heading(text):\n","    \"\"\"Split the report into sections\"\"\"\n","    start = 0\n","    for matcher in SECTION_TITLES.finditer(text):\n","        # add last\n","        end = matcher.start()\n","        if end != start:\n","            section = text[start:end].strip()\n","            if section:\n","                yield section\n","\n","        # add title\n","        start = end\n","        end = matcher.end()\n","        if end != start:\n","            section = text[start:end].strip()\n","            if section:\n","                yield section\n","\n","        start = end\n","\n","    # add last piece\n","    end = len(text)\n","    if start < end:\n","        section = text[start:end].strip()\n","        if section:\n","            yield section\n","\n","def find_end(text):\n","    \"\"\"Find the end of the report.\"\"\"\n","    ends = [len(text)]\n","    patterns = [\n","        re.compile(r'BY ELECTRONICALLY SIGNING THIS REPORT', re.I),\n","        re.compile(r'\\n {3,}DR.', re.I),\n","        re.compile(r'[ ]{1,}RADLINE ', re.I),\n","        re.compile(r'.*electronically signed on', re.I),\n","        re.compile(r'M\\[0KM\\[0KM')\n","    ]\n","    for pattern in patterns:\n","        matchobj = pattern.search(text)\n","        if matchobj:\n","            ends.append(matchobj.start())\n","    return min(ends)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Xk-uBK2Nuk3"},"outputs":[],"source":["DATAPATH = \"data/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAWBwNbHNuk3"},"outputs":[],"source":["# load MIMIC III raw data\n","def load_raw_data(raw_data_dir):\n","  \"\"\"Loads raw data to dataframe/numpy array/tensor\"\"\"\n","  admission_df = pd.read_csv(os.path.join(raw_data_dir, 'ADMISSIONS.csv'))\n","  events_df = pd.read_csv(os.path.join(raw_data_dir, 'NOTEEVENTS.csv'), low_memory = False)\n","  icu_stay_df = pd.read_csv(os.path.join(raw_data_dir, 'ICUSTAYS.csv'))\n","\n","  #print(len(admission_df))\n","  #print(len(events_df))\n","  #print(len(icu_stay_df))\n","\n","  return [admission_df, events_df, icu_stay_df]\n","\n","raw_data = load_raw_data(DATAPATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYI09N6gNuk3"},"outputs":[],"source":["# Calculate statistics of MIMIC-III data\n","\n","def calculate_stats(raw_data):\n","  \"\"\"Implement this function to calculate the statistics.\n","  It is encouraged to print out the results.\"\"\"\n","  admission_data = raw_data[0]\n","  note_events_data = raw_data[1]\n","  icu_stay_data = raw_data[2]\n","  print(\"There are a total of \", len(admission_data), \" Admission Data\")\n","  print(\"There are a total of \", len(icu_stay_data), \" ICU Stay Data\")\n","\n","  ## Select Clinical Notes\n","  note_events_data.groupby(note_events_data.CATEGORY).agg(['count'])\n","  note_categories = note_events_data.groupby(note_events_data.CATEGORY).agg(['count']).index\n","  selected_note = []\n","  for category in list(note_categories):\n","    if category != 'Discharge summary':\n","      selected_note.append(category)\n","\n","  ## Create sub notes based on category\n","  sub_notes = note_events_data[note_events_data.CATEGORY.isin(selected_note)]\n","  sub_notes.shape\n","\n","  ## Handle missing chart\n","  missing_chart_notes_idx = []\n","  for note in sub_notes.itertuples():\n","    if isinstance(note.CHARTTIME, str):\n","      continue\n","    if np.isnan(note.CHARTTIME):\n","      missing_chart_notes_idx.append(note.Index)\n","  print(\"{} of notes missing charttime.\".format(len(missing_chart_notes_idx)))\n","  sub_notes.drop(missing_chart_notes_idx, inplace = True)\n","  print(\"sub_notes shape: \", sub_notes.shape)\n","\n","  ## Select based on Time Limit (24 hours)\n","  MIMIC_EXTRACT_DATA = os.path.join(DATAPATH, 'all_hourly_data.h5')\n","  statistic = pd.read_hdf(MIMIC_EXTRACT_DATA, 'patients')\n","  print(\"MIMIC-EXTRACT DATA (Patients Num & Hospital Admission & ICU Admission): \", len(statistic))\n","  TIMELIMIT = 1 ## 1 day\n","  statistic.shape\n","  statistic.head()\n","  new_stats = statistic.reset_index()\n","  new_stats.rename(columns = {\"subject_id\": \"SUBJECT_ID\", \"hadm_id\": \"HADM_ID\"}, inplace = True)\n","  print(\"new_stats shape: \", new_stats.shape, \"\\nsub_notes shape: \", sub_notes.shape)\n","  df_adm_notes = pd.merge(sub_notes[['ROW_ID','SUBJECT_ID','HADM_ID','CHARTTIME', 'CATEGORY', 'TEXT']],\n","                          new_stats[['SUBJECT_ID','HADM_ID','icustay_id','age','admittime','dischtime', 'deathtime', 'intime', 'outtime', 'los_icu', 'mort_icu', 'mort_hosp', 'hospital_expire_flag', 'hospstay_seq', 'max_hours']],\n","                          on = ['SUBJECT_ID'],\n","                          how = 'left')\n","  df_adm_notes.head()\n","  df_adm_notes['CHARTTIME'] = pd.to_datetime(df_adm_notes['CHARTTIME'])\n","  df_less_n = df_adm_notes[((df_adm_notes['CHARTTIME'] - df_adm_notes['intime']).dt.total_seconds() / (24*60*60)) < TIMELIMIT]\n","  print(\"df_less_n.shape: \", df_less_n.shape)\n","\n","  # Save clinical notes\n","  pd.to_pickle(df_less_n, os.path.join(DATAPATH, 'sub_notes.p'))\n","  return df_less_n\n","\n","state = calculate_stats(raw_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lC7u4c2Nuk3"},"outputs":[],"source":["# Process raw data\n","\n","def process_data(raw_data):\n","  \"\"\"Preprocess the text in the clinical notes\"\"\"\n","  clinical_notes = pd.read_pickle(os.path.join(DATAPATH, 'sub_notes.p'))\n","  print(\"Clinical notes shape: \", clinical_notes.shape)\n","\n","  eliminate_notes = clinical_notes[clinical_notes.SUBJECT_ID.notnull()]\n","  eliminate_notes = eliminate_notes[eliminate_notes.CHARTTIME.notnull()]\n","  eliminate_notes = eliminate_notes[eliminate_notes.TEXT.notnull()]\n","\n","  eliminate_notes = eliminate_notes[['SUBJECT_ID', 'HADM_ID_y', 'CHARTTIME', 'TEXT']]\n","  eliminate_notes['preprocessed_text'] = None\n","\n","  for notes in eliminate_notes.itertuples():\n","    text = notes.TEXT\n","    eliminate_notes.at[notes.Index, 'preprocessed_text'] = getSentences(text)\n","\n","  pd.to_pickle(eliminate_notes, os.path.join(DATAPATH, 'preprocessed_notes.p'))\n","  print(\"Preprocessed Data shape: \", eliminate_notes.shape)\n","  return eliminate_notes\n","\n","processed_data = process_data(raw_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJYDAfnzNuk3"},"outputs":[],"source":["processed_data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmLAeRg6Nuk4"},"outputs":[],"source":["processed_data.head()"]}],"metadata":{"colab":{"provenance":[{"file_id":"1MGxB_J2TvhAANcQG8VNMvQp1QdQrcxWb","timestamp":1711309692552},{"file_id":"1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5","timestamp":1709153069464}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}