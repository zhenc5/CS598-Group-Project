{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "#import glove\n",
    "#from glove import Corpus\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, Activation, Concatenate, LSTM, GRU\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
    "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.backend import set_session, clear_session, get_session\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training the multimodal model\n",
    "\n",
    "def reset_keras(model):\n",
    "    \"\"\"reset keras session\"\"\"\n",
    "    sess = get_session()\n",
    "    clear_session()\n",
    "    sess.close()\n",
    "    sess = get_session()\n",
    "\n",
    "    try:\n",
    "        del model # this is from global space - change this as you need\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    gc.collect() # if it's done something you should see a number being outputted\n",
    "\n",
    "def create_dataset(dict_of_ner):\n",
    "    \"\"\"create the dataset\"\"\"\n",
    "    temp_data = []\n",
    "    for k, v in sorted(dict_of_ner.items()):\n",
    "        temp = []\n",
    "        for embed in v:\n",
    "            temp.append(embed)\n",
    "        temp_data.append(np.mean(temp, axis = 0)) \n",
    "    return np.asarray(temp_data)\n",
    "\n",
    "def make_prediction_multi_avg(model, test_data):\n",
    "    \"\"\"make model predictions\"\"\"\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def save_scores_multi_avg(predictions, probs, ground_truth,                          \n",
    "                          embed_name, problem_type, iteration, hidden_unit_size,                         \n",
    "                          sequence_name, type_of_ner):\n",
    "    \"\"\"save metrics of model\"\"\"\n",
    "    auc = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    result_dict = {}    \n",
    "    result_dict['auc'] = auc\n",
    "    result_dict['auprc'] = auprc\n",
    "    result_dict['acc'] = acc\n",
    "    result_dict['F1'] = F1\n",
    "    \n",
    "    result_path = \"results/multimodal\"\n",
    "    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n",
    "    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-avg-.p\"\n",
    "    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
    "\n",
    "    print(auc, auprc, acc, F1)\n",
    "    \n",
    "def avg_ner_model(layer_name, number_of_unit, embedding_name):\n",
    "    \"\"\"define the model specifications\"\"\"\n",
    "\n",
    "    #if embedding_name == \"concat\":\n",
    "    #    input_dimension = 200\n",
    "    #else:\n",
    "    #    input_dimension = 100\n",
    "    input_dimension = 100\n",
    "\n",
    "    sequence_input = Input(shape=(24,104))\n",
    "\n",
    "    input_avg = Input(shape=(input_dimension, ), name = \"avg\")        \n",
    "#     x_1 = Dense(256, activation='relu')(input_avg)\n",
    "#     x_1 = Dropout(0.3)(x_1)\n",
    "    \n",
    "    if layer_name == \"GRU\":\n",
    "        x = GRU(number_of_unit)(sequence_input)\n",
    "    elif layer_name == \"LSTM\":\n",
    "        x = LSTM(number_of_unit)(sequence_input)\n",
    "\n",
    "    x = keras.layers.Concatenate()([x, input_avg])\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    \n",
    "    #logits_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
    "    logits_regularizer = keras.regularizers.l2(0.01)\n",
    "    \n",
    "    #preds = Dense(1, activation='sigmoid',use_bias=False,\n",
    "    #                     kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "    #              kernel_regularizer=logits_regularizer)(x)\n",
    "    preds = Dense(1, activation='sigmoid',use_bias=False,\n",
    "                         kernel_initializer=tf.keras.initializers.GlorotUniform(), \n",
    "                  kernel_regularizer=logits_regularizer)(x)\n",
    "    \n",
    "    \n",
    "    opt = Adam(lr=0.001, decay = 0.01)\n",
    "    model = Model(inputs=[sequence_input, input_avg], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "\n",
    "type_of_ner = \"new\"\n",
    "\n",
    "x_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\n",
    "x_dev_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\n",
    "x_test_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\n",
    "y_dev = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\n",
    "y_test = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")\n",
    "\n",
    "ner_word2vec = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\n",
    "ner_fasttext = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\n",
    "ner_concat = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n",
    "\n",
    "train_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\n",
    "dev_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\n",
    "test_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:  GRU\n",
      "Hidden unit:  256\n",
      "Embedding:  word2vec\n",
      "=============================\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "Epoch 1/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.2842 - acc: 0.9058\n",
      "Epoch 1: val_loss improved from inf to 0.24494, saving model to avg-word2vec-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 28s 100ms/step - loss: 0.2842 - acc: 0.9058 - val_loss: 0.2449 - val_acc: 0.9161\n",
      "Epoch 2/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2444 - acc: 0.9154\n",
      "Epoch 2: val_loss improved from 0.24494 to 0.24015, saving model to avg-word2vec-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 21s 85ms/step - loss: 0.2443 - acc: 0.9155 - val_loss: 0.2402 - val_acc: 0.9188\n",
      "Epoch 3/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.2294 - acc: 0.9201\n",
      "Epoch 3: val_loss improved from 0.24015 to 0.23655, saving model to avg-word2vec-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 21s 86ms/step - loss: 0.2294 - acc: 0.9201 - val_loss: 0.2366 - val_acc: 0.9143\n",
      "Epoch 4/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2217 - acc: 0.9226\n",
      "Epoch 4: val_loss improved from 0.23655 to 0.23517, saving model to avg-word2vec-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 20s 84ms/step - loss: 0.2218 - acc: 0.9226 - val_loss: 0.2352 - val_acc: 0.9156\n",
      "Epoch 5/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2163 - acc: 0.9243\n",
      "Epoch 5: val_loss improved from 0.23517 to 0.23378, saving model to avg-word2vec-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 19s 79ms/step - loss: 0.2165 - acc: 0.9243 - val_loss: 0.2338 - val_acc: 0.9179\n",
      "Epoch 6/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9256\n",
      "Epoch 6: val_loss did not improve from 0.23378\n",
      "244/244 [==============================] - 20s 81ms/step - loss: 0.2121 - acc: 0.9256 - val_loss: 0.2345 - val_acc: 0.9161\n",
      "Epoch 7/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.2083 - acc: 0.9264\n",
      "Epoch 7: val_loss improved from 0.23378 to 0.23216, saving model to avg-word2vec-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 21s 85ms/step - loss: 0.2083 - acc: 0.9264 - val_loss: 0.2322 - val_acc: 0.9179\n",
      "Epoch 8/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2063 - acc: 0.9279\n",
      "Epoch 8: val_loss did not improve from 0.23216\n",
      "244/244 [==============================] - 20s 84ms/step - loss: 0.2063 - acc: 0.9277 - val_loss: 0.2343 - val_acc: 0.9165\n",
      "Epoch 9/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9290\n",
      "Epoch 9: val_loss improved from 0.23216 to 0.23203, saving model to avg-word2vec-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 21s 87ms/step - loss: 0.2010 - acc: 0.9290 - val_loss: 0.2320 - val_acc: 0.9156\n",
      "Epoch 10/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.9293\n",
      "Epoch 10: val_loss did not improve from 0.23203\n",
      "244/244 [==============================] - 21s 88ms/step - loss: 0.2004 - acc: 0.9293 - val_loss: 0.2335 - val_acc: 0.9188\n",
      "Epoch 11/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.1982 - acc: 0.9297\n",
      "Epoch 11: val_loss did not improve from 0.23203\n",
      "244/244 [==============================] - 21s 85ms/step - loss: 0.1982 - acc: 0.9297 - val_loss: 0.2336 - val_acc: 0.9161\n",
      "Epoch 12/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.1955 - acc: 0.9299\n",
      "Epoch 12: val_loss did not improve from 0.23203\n",
      "244/244 [==============================] - 21s 88ms/step - loss: 0.1955 - acc: 0.9299 - val_loss: 0.2351 - val_acc: 0.9152\n",
      "Epoch 13/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.1944 - acc: 0.9302\n",
      "Epoch 13: val_loss did not improve from 0.23203\n",
      "244/244 [==============================] - 21s 86ms/step - loss: 0.1944 - acc: 0.9302 - val_loss: 0.2346 - val_acc: 0.9143\n",
      "Epoch 14/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1937 - acc: 0.9307\n",
      "Epoch 14: val_loss did not improve from 0.23203\n",
      "244/244 [==============================] - 20s 83ms/step - loss: 0.1936 - acc: 0.9308 - val_loss: 0.2361 - val_acc: 0.9161\n",
      "139/139 [==============================] - 4s 26ms/step\n",
      "0.8836159439806087 0.5942203448081539 0.9178733031674208 0.47007299270072994\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "Epoch 1/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.9370\n",
      "Epoch 1: val_loss improved from inf to 0.17745, saving model to avg-word2vec-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 29s 109ms/step - loss: 0.2107 - acc: 0.9370 - val_loss: 0.1775 - val_acc: 0.9436\n",
      "Epoch 2/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1758 - acc: 0.9448\n",
      "Epoch 2: val_loss improved from 0.17745 to 0.17345, saving model to avg-word2vec-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 24s 100ms/step - loss: 0.1757 - acc: 0.9448 - val_loss: 0.1734 - val_acc: 0.9404\n",
      "Epoch 3/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9464\n",
      "Epoch 3: val_loss improved from 0.17345 to 0.17240, saving model to avg-word2vec-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 25s 101ms/step - loss: 0.1633 - acc: 0.9463 - val_loss: 0.1724 - val_acc: 0.9431\n",
      "Epoch 4/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9482\n",
      "Epoch 4: val_loss improved from 0.17240 to 0.17120, saving model to avg-word2vec-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 24s 97ms/step - loss: 0.1583 - acc: 0.9481 - val_loss: 0.1712 - val_acc: 0.9422\n",
      "Epoch 5/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9487\n",
      "Epoch 5: val_loss did not improve from 0.17120\n",
      "244/244 [==============================] - 24s 100ms/step - loss: 0.1542 - acc: 0.9487 - val_loss: 0.1715 - val_acc: 0.9440\n",
      "Epoch 6/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.9501\n",
      "Epoch 6: val_loss did not improve from 0.17120\n",
      "244/244 [==============================] - 24s 98ms/step - loss: 0.1493 - acc: 0.9502 - val_loss: 0.1718 - val_acc: 0.9445\n",
      "Epoch 7/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.9509\n",
      "Epoch 7: val_loss did not improve from 0.17120\n",
      "244/244 [==============================] - 24s 99ms/step - loss: 0.1462 - acc: 0.9508 - val_loss: 0.1750 - val_acc: 0.9454\n",
      "Epoch 8/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9522\n",
      "Epoch 8: val_loss did not improve from 0.17120\n",
      "244/244 [==============================] - 24s 97ms/step - loss: 0.1442 - acc: 0.9523 - val_loss: 0.1718 - val_acc: 0.9440\n",
      "Epoch 9/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9530\n",
      "Epoch 9: val_loss did not improve from 0.17120\n",
      "244/244 [==============================] - 24s 98ms/step - loss: 0.1419 - acc: 0.9530 - val_loss: 0.1733 - val_acc: 0.9454\n",
      "139/139 [==============================] - 4s 24ms/step\n",
      "0.8915010011989856 0.5307574179240933 0.9445701357466063 0.4863731656184486\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "Epoch 1/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.6852 - acc: 0.6216\n",
      "Epoch 1: val_loss improved from inf to 0.63300, saving model to avg-word2vec-los_3-best_model.keras\n",
      "244/244 [==============================] - 32s 119ms/step - loss: 0.6853 - acc: 0.6214 - val_loss: 0.6330 - val_acc: 0.6724\n",
      "Epoch 2/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.6383 - acc: 0.6575\n",
      "Epoch 2: val_loss improved from 0.63300 to 0.62762, saving model to avg-word2vec-los_3-best_model.keras\n",
      "244/244 [==============================] - 28s 114ms/step - loss: 0.6381 - acc: 0.6577 - val_loss: 0.6276 - val_acc: 0.6805\n",
      "Epoch 3/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.6205 - acc: 0.6730\n",
      "Epoch 3: val_loss improved from 0.62762 to 0.62160, saving model to avg-word2vec-los_3-best_model.keras\n",
      "244/244 [==============================] - 27s 110ms/step - loss: 0.6205 - acc: 0.6730 - val_loss: 0.6216 - val_acc: 0.6755\n",
      "Epoch 4/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.6125 - acc: 0.6808\n",
      "Epoch 4: val_loss improved from 0.62160 to 0.61879, saving model to avg-word2vec-los_3-best_model.keras\n",
      "244/244 [==============================] - 28s 113ms/step - loss: 0.6125 - acc: 0.6808 - val_loss: 0.6188 - val_acc: 0.6728\n",
      "Epoch 5/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.6935\n",
      "Epoch 5: val_loss improved from 0.61879 to 0.61737, saving model to avg-word2vec-los_3-best_model.keras\n",
      "244/244 [==============================] - 27s 111ms/step - loss: 0.6031 - acc: 0.6934 - val_loss: 0.6174 - val_acc: 0.6773\n",
      "Epoch 6/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5970 - acc: 0.6944\n",
      "Epoch 6: val_loss improved from 0.61737 to 0.61673, saving model to avg-word2vec-los_3-best_model.keras\n",
      "244/244 [==============================] - 27s 110ms/step - loss: 0.5969 - acc: 0.6945 - val_loss: 0.6167 - val_acc: 0.6805\n",
      "Epoch 7/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5928 - acc: 0.6997\n",
      "Epoch 7: val_loss did not improve from 0.61673\n",
      "244/244 [==============================] - 27s 111ms/step - loss: 0.5928 - acc: 0.6997 - val_loss: 0.6176 - val_acc: 0.6782\n",
      "Epoch 8/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5887 - acc: 0.7013\n",
      "Epoch 8: val_loss did not improve from 0.61673\n",
      "244/244 [==============================] - 27s 112ms/step - loss: 0.5888 - acc: 0.7011 - val_loss: 0.6168 - val_acc: 0.6760\n",
      "Epoch 9/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5849 - acc: 0.7047\n",
      "Epoch 9: val_loss did not improve from 0.61673\n",
      "244/244 [==============================] - 27s 111ms/step - loss: 0.5850 - acc: 0.7047 - val_loss: 0.6176 - val_acc: 0.6782\n",
      "Epoch 10/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.5828 - acc: 0.7069\n",
      "Epoch 10: val_loss improved from 0.61673 to 0.61649, saving model to avg-word2vec-los_3-best_model.keras\n",
      "244/244 [==============================] - 27s 111ms/step - loss: 0.5828 - acc: 0.7069 - val_loss: 0.6165 - val_acc: 0.6778\n",
      "Epoch 11/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5776 - acc: 0.7129\n",
      "Epoch 11: val_loss did not improve from 0.61649\n",
      "244/244 [==============================] - 25s 104ms/step - loss: 0.5775 - acc: 0.7130 - val_loss: 0.6171 - val_acc: 0.6728\n",
      "Epoch 12/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5759 - acc: 0.7138\n",
      "Epoch 12: val_loss did not improve from 0.61649\n",
      "244/244 [==============================] - 31s 126ms/step - loss: 0.5758 - acc: 0.7139 - val_loss: 0.6178 - val_acc: 0.6710\n",
      "Epoch 13/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5754 - acc: 0.7139\n",
      "Epoch 13: val_loss did not improve from 0.61649\n",
      "244/244 [==============================] - 24s 98ms/step - loss: 0.5753 - acc: 0.7139 - val_loss: 0.6171 - val_acc: 0.6755\n",
      "Epoch 14/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5712 - acc: 0.7179\n",
      "Epoch 14: val_loss did not improve from 0.61649\n",
      "244/244 [==============================] - 24s 97ms/step - loss: 0.5711 - acc: 0.7180 - val_loss: 0.6182 - val_acc: 0.6706\n",
      "Epoch 15/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5674 - acc: 0.7193\n",
      "Epoch 15: val_loss did not improve from 0.61649\n",
      "244/244 [==============================] - 24s 98ms/step - loss: 0.5673 - acc: 0.7194 - val_loss: 0.6175 - val_acc: 0.6787\n",
      "139/139 [==============================] - 2s 10ms/step\n",
      "0.7040403297026788 0.6431130108632286 0.6647058823529411 0.5748709122203098\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "Epoch 1/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.9167\n",
      "Epoch 1: val_loss improved from inf to 0.27371, saving model to avg-word2vec-los_7-best_model.keras\n",
      "244/244 [==============================] - 14s 51ms/step - loss: 0.2958 - acc: 0.9167 - val_loss: 0.2737 - val_acc: 0.9242\n",
      "Epoch 2/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2602 - acc: 0.9217\n",
      "Epoch 2: val_loss improved from 0.27371 to 0.26764, saving model to avg-word2vec-los_7-best_model.keras\n",
      "244/244 [==============================] - 11s 44ms/step - loss: 0.2603 - acc: 0.9217 - val_loss: 0.2676 - val_acc: 0.9242\n",
      "Epoch 3/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.2508 - acc: 0.9221\n",
      "Epoch 3: val_loss did not improve from 0.26764\n",
      "244/244 [==============================] - 12s 49ms/step - loss: 0.2508 - acc: 0.9221 - val_loss: 0.2688 - val_acc: 0.9237\n",
      "Epoch 4/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2442 - acc: 0.9233\n",
      "Epoch 4: val_loss improved from 0.26764 to 0.26652, saving model to avg-word2vec-los_7-best_model.keras\n",
      "244/244 [==============================] - 12s 49ms/step - loss: 0.2440 - acc: 0.9234 - val_loss: 0.2665 - val_acc: 0.9242\n",
      "Epoch 5/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2407 - acc: 0.9232\n",
      "Epoch 5: val_loss improved from 0.26652 to 0.26611, saving model to avg-word2vec-los_7-best_model.keras\n",
      "244/244 [==============================] - 12s 48ms/step - loss: 0.2406 - acc: 0.9233 - val_loss: 0.2661 - val_acc: 0.9242\n",
      "Epoch 6/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2361 - acc: 0.9239\n",
      "Epoch 6: val_loss did not improve from 0.26611\n",
      "244/244 [==============================] - 12s 49ms/step - loss: 0.2363 - acc: 0.9238 - val_loss: 0.2665 - val_acc: 0.9237\n",
      "Epoch 7/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2338 - acc: 0.9241\n",
      "Epoch 7: val_loss improved from 0.26611 to 0.26536, saving model to avg-word2vec-los_7-best_model.keras\n",
      "244/244 [==============================] - 12s 49ms/step - loss: 0.2338 - acc: 0.9241 - val_loss: 0.2654 - val_acc: 0.9233\n",
      "Epoch 8/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2317 - acc: 0.9244\n",
      "Epoch 8: val_loss did not improve from 0.26536\n",
      "244/244 [==============================] - 12s 49ms/step - loss: 0.2316 - acc: 0.9245 - val_loss: 0.2662 - val_acc: 0.9237\n",
      "Epoch 9/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9239\n",
      "Epoch 9: val_loss did not improve from 0.26536\n",
      "244/244 [==============================] - 12s 49ms/step - loss: 0.2292 - acc: 0.9239 - val_loss: 0.2667 - val_acc: 0.9233\n",
      "Epoch 10/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.2271 - acc: 0.9246\n",
      "Epoch 10: val_loss did not improve from 0.26536\n",
      "244/244 [==============================] - 12s 50ms/step - loss: 0.2271 - acc: 0.9246 - val_loss: 0.2662 - val_acc: 0.9237\n",
      "Epoch 11/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.2243 - acc: 0.9255\n",
      "Epoch 11: val_loss did not improve from 0.26536\n",
      "244/244 [==============================] - 12s 50ms/step - loss: 0.2243 - acc: 0.9255 - val_loss: 0.2660 - val_acc: 0.9242\n",
      "Epoch 12/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2240 - acc: 0.9255\n",
      "Epoch 12: val_loss did not improve from 0.26536\n",
      "244/244 [==============================] - 12s 50ms/step - loss: 0.2240 - acc: 0.9255 - val_loss: 0.2660 - val_acc: 0.9242\n",
      "139/139 [==============================] - 1s 8ms/step\n",
      "0.7388498101677293 0.23138626107332042 0.919683257918552 0.02203856749311295\n",
      "Embedding:  fasttext\n",
      "=============================\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "Epoch 1/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2908 - acc: 0.9023\n",
      "Epoch 1: val_loss improved from inf to 0.24480, saving model to avg-fasttext-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 21s 81ms/step - loss: 0.2906 - acc: 0.9023 - val_loss: 0.2448 - val_acc: 0.9138\n",
      "Epoch 2/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2422 - acc: 0.9169\n",
      "Epoch 2: val_loss improved from 0.24480 to 0.23638, saving model to avg-fasttext-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 19s 78ms/step - loss: 0.2426 - acc: 0.9168 - val_loss: 0.2364 - val_acc: 0.9147\n",
      "Epoch 3/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2274 - acc: 0.9201\n",
      "Epoch 3: val_loss improved from 0.23638 to 0.23523, saving model to avg-fasttext-mort_hosp-best_model.keras\n",
      "244/244 [==============================] - 20s 81ms/step - loss: 0.2273 - acc: 0.9201 - val_loss: 0.2352 - val_acc: 0.9156\n",
      "Epoch 4/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2218 - acc: 0.9220\n",
      "Epoch 4: val_loss did not improve from 0.23523\n",
      "244/244 [==============================] - 20s 81ms/step - loss: 0.2217 - acc: 0.9221 - val_loss: 0.2356 - val_acc: 0.9143\n",
      "Epoch 5/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2162 - acc: 0.9244\n",
      "Epoch 5: val_loss did not improve from 0.23523\n",
      "244/244 [==============================] - 20s 81ms/step - loss: 0.2161 - acc: 0.9245 - val_loss: 0.2366 - val_acc: 0.9134\n",
      "Epoch 6/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9230\n",
      "Epoch 6: val_loss did not improve from 0.23523\n",
      "244/244 [==============================] - 20s 81ms/step - loss: 0.2125 - acc: 0.9229 - val_loss: 0.2363 - val_acc: 0.9125\n",
      "Epoch 7/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2071 - acc: 0.9276\n",
      "Epoch 7: val_loss did not improve from 0.23523\n",
      "244/244 [==============================] - 20s 82ms/step - loss: 0.2071 - acc: 0.9276 - val_loss: 0.2365 - val_acc: 0.9152\n",
      "Epoch 8/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9260\n",
      "Epoch 8: val_loss did not improve from 0.23523\n",
      "244/244 [==============================] - 20s 81ms/step - loss: 0.2058 - acc: 0.9260 - val_loss: 0.2359 - val_acc: 0.9134\n",
      "139/139 [==============================] - 2s 11ms/step\n",
      "0.8831290061944519 0.593273257448929 0.918552036199095 0.4444444444444444\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "Epoch 1/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2283 - acc: 0.9318\n",
      "Epoch 1: val_loss improved from inf to 0.18059, saving model to avg-fasttext-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 18s 68ms/step - loss: 0.2283 - acc: 0.9317 - val_loss: 0.1806 - val_acc: 0.9413\n",
      "Epoch 2/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9442\n",
      "Epoch 2: val_loss improved from 0.18059 to 0.17399, saving model to avg-fasttext-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 14s 56ms/step - loss: 0.1792 - acc: 0.9442 - val_loss: 0.1740 - val_acc: 0.9436\n",
      "Epoch 3/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1680 - acc: 0.9466\n",
      "Epoch 3: val_loss improved from 0.17399 to 0.17202, saving model to avg-fasttext-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 14s 59ms/step - loss: 0.1683 - acc: 0.9464 - val_loss: 0.1720 - val_acc: 0.9431\n",
      "Epoch 4/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1595 - acc: 0.9489\n",
      "Epoch 4: val_loss did not improve from 0.17202\n",
      "244/244 [==============================] - 14s 59ms/step - loss: 0.1595 - acc: 0.9489 - val_loss: 0.1725 - val_acc: 0.9436\n",
      "Epoch 5/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.1548 - acc: 0.9503\n",
      "Epoch 5: val_loss improved from 0.17202 to 0.17074, saving model to avg-fasttext-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 0.1548 - acc: 0.9503 - val_loss: 0.1707 - val_acc: 0.9445\n",
      "Epoch 6/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.1520 - acc: 0.9498\n",
      "Epoch 6: val_loss improved from 0.17074 to 0.17060, saving model to avg-fasttext-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 0.1520 - acc: 0.9498 - val_loss: 0.1706 - val_acc: 0.9440\n",
      "Epoch 7/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.1471 - acc: 0.9518\n",
      "Epoch 7: val_loss did not improve from 0.17060\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 0.1471 - acc: 0.9518 - val_loss: 0.1707 - val_acc: 0.9449\n",
      "Epoch 8/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9517\n",
      "Epoch 8: val_loss improved from 0.17060 to 0.17012, saving model to avg-fasttext-mort_icu-best_model.keras\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 0.1450 - acc: 0.9518 - val_loss: 0.1701 - val_acc: 0.9449\n",
      "Epoch 9/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9523\n",
      "Epoch 9: val_loss did not improve from 0.17012\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 0.1426 - acc: 0.9523 - val_loss: 0.1713 - val_acc: 0.9449\n",
      "Epoch 10/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.1414 - acc: 0.9534\n",
      "Epoch 10: val_loss did not improve from 0.17012\n",
      "244/244 [==============================] - 14s 59ms/step - loss: 0.1414 - acc: 0.9534 - val_loss: 0.1714 - val_acc: 0.9449\n",
      "Epoch 11/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.9539\n",
      "Epoch 11: val_loss did not improve from 0.17012\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 0.1384 - acc: 0.9539 - val_loss: 0.1709 - val_acc: 0.9454\n",
      "Epoch 12/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9552\n",
      "Epoch 12: val_loss did not improve from 0.17012\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 0.1370 - acc: 0.9552 - val_loss: 0.1709 - val_acc: 0.9454\n",
      "Epoch 13/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9558\n",
      "Epoch 13: val_loss did not improve from 0.17012\n",
      "244/244 [==============================] - 14s 58ms/step - loss: 0.1348 - acc: 0.9558 - val_loss: 0.1709 - val_acc: 0.9436\n",
      "139/139 [==============================] - 2s 9ms/step\n",
      "0.8922699158870194 0.5290632593598015 0.9434389140271493 0.4588744588744589\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "Epoch 1/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.6836 - acc: 0.6190\n",
      "Epoch 1: val_loss improved from inf to 0.62870, saving model to avg-fasttext-los_3-best_model.keras\n",
      "244/244 [==============================] - 18s 67ms/step - loss: 0.6834 - acc: 0.6192 - val_loss: 0.6287 - val_acc: 0.6814\n",
      "Epoch 2/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.6324 - acc: 0.6613\n",
      "Epoch 2: val_loss improved from 0.62870 to 0.62195, saving model to avg-fasttext-los_3-best_model.keras\n",
      "244/244 [==============================] - 14s 57ms/step - loss: 0.6324 - acc: 0.6613 - val_loss: 0.6219 - val_acc: 0.6805\n",
      "Epoch 3/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.6186 - acc: 0.6791\n",
      "Epoch 3: val_loss did not improve from 0.62195\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.6186 - acc: 0.6792 - val_loss: 0.6268 - val_acc: 0.6737\n",
      "Epoch 4/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.6100 - acc: 0.6832\n",
      "Epoch 4: val_loss improved from 0.62195 to 0.62011, saving model to avg-fasttext-los_3-best_model.keras\n",
      "244/244 [==============================] - 15s 61ms/step - loss: 0.6101 - acc: 0.6832 - val_loss: 0.6201 - val_acc: 0.6755\n",
      "Epoch 5/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.6020 - acc: 0.6890\n",
      "Epoch 5: val_loss improved from 0.62011 to 0.61459, saving model to avg-fasttext-los_3-best_model.keras\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.6020 - acc: 0.6890 - val_loss: 0.6146 - val_acc: 0.6819\n",
      "Epoch 6/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5964 - acc: 0.6912\n",
      "Epoch 6: val_loss did not improve from 0.61459\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.5964 - acc: 0.6912 - val_loss: 0.6156 - val_acc: 0.6828\n",
      "Epoch 7/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5935 - acc: 0.6992\n",
      "Epoch 7: val_loss improved from 0.61459 to 0.61276, saving model to avg-fasttext-los_3-best_model.keras\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.5934 - acc: 0.6994 - val_loss: 0.6128 - val_acc: 0.6868\n",
      "Epoch 8/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5884 - acc: 0.6997\n",
      "Epoch 8: val_loss did not improve from 0.61276\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.5882 - acc: 0.6999 - val_loss: 0.6161 - val_acc: 0.6841\n",
      "Epoch 9/100\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.5849 - acc: 0.7025\n",
      "Epoch 9: val_loss did not improve from 0.61276\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.5849 - acc: 0.7025 - val_loss: 0.6149 - val_acc: 0.6841\n",
      "Epoch 10/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5810 - acc: 0.7069\n",
      "Epoch 10: val_loss did not improve from 0.61276\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.5811 - acc: 0.7066 - val_loss: 0.6178 - val_acc: 0.6737\n",
      "Epoch 11/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5807 - acc: 0.7112\n",
      "Epoch 11: val_loss did not improve from 0.61276\n",
      "244/244 [==============================] - 15s 60ms/step - loss: 0.5808 - acc: 0.7111 - val_loss: 0.6161 - val_acc: 0.6801\n",
      "Epoch 12/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.5764 - acc: 0.7130\n",
      "Epoch 12: val_loss did not improve from 0.61276\n",
      "244/244 [==============================] - 15s 61ms/step - loss: 0.5764 - acc: 0.7132 - val_loss: 0.6139 - val_acc: 0.6801\n",
      "139/139 [==============================] - 2s 9ms/step\n",
      "0.7067940545802446 0.6429023153071477 0.6687782805429864 0.5711775043936731\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "Epoch 1/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2896 - acc: 0.9180\n",
      "Epoch 1: val_loss improved from inf to 0.27396, saving model to avg-fasttext-los_7-best_model.keras\n",
      "244/244 [==============================] - 17s 66ms/step - loss: 0.2899 - acc: 0.9178 - val_loss: 0.2740 - val_acc: 0.9237\n",
      "Epoch 2/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9216\n",
      "Epoch 2: val_loss improved from 0.27396 to 0.26656, saving model to avg-fasttext-los_7-best_model.keras\n",
      "244/244 [==============================] - 17s 68ms/step - loss: 0.2565 - acc: 0.9216 - val_loss: 0.2666 - val_acc: 0.9242\n",
      "Epoch 3/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9226\n",
      "Epoch 3: val_loss did not improve from 0.26656\n",
      "244/244 [==============================] - 17s 68ms/step - loss: 0.2475 - acc: 0.9226 - val_loss: 0.2666 - val_acc: 0.9237\n",
      "Epoch 4/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2416 - acc: 0.9237\n",
      "Epoch 4: val_loss improved from 0.26656 to 0.26640, saving model to avg-fasttext-los_7-best_model.keras\n",
      "244/244 [==============================] - 17s 69ms/step - loss: 0.2416 - acc: 0.9236 - val_loss: 0.2664 - val_acc: 0.9237\n",
      "Epoch 5/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2377 - acc: 0.9232\n",
      "Epoch 5: val_loss improved from 0.26640 to 0.26528, saving model to avg-fasttext-los_7-best_model.keras\n",
      "244/244 [==============================] - 17s 69ms/step - loss: 0.2379 - acc: 0.9231 - val_loss: 0.2653 - val_acc: 0.9237\n",
      "Epoch 6/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9247\n",
      "Epoch 6: val_loss did not improve from 0.26528\n",
      "244/244 [==============================] - 17s 68ms/step - loss: 0.2345 - acc: 0.9247 - val_loss: 0.2668 - val_acc: 0.9228\n",
      "Epoch 7/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2313 - acc: 0.9244\n",
      "Epoch 7: val_loss improved from 0.26528 to 0.26444, saving model to avg-fasttext-los_7-best_model.keras\n",
      "244/244 [==============================] - 17s 69ms/step - loss: 0.2312 - acc: 0.9245 - val_loss: 0.2644 - val_acc: 0.9233\n",
      "Epoch 8/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2291 - acc: 0.9246\n",
      "Epoch 8: val_loss did not improve from 0.26444\n",
      "244/244 [==============================] - 17s 69ms/step - loss: 0.2290 - acc: 0.9246 - val_loss: 0.2656 - val_acc: 0.9224\n",
      "Epoch 9/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2268 - acc: 0.9250\n",
      "Epoch 9: val_loss did not improve from 0.26444\n",
      "244/244 [==============================] - 17s 69ms/step - loss: 0.2269 - acc: 0.9250 - val_loss: 0.2656 - val_acc: 0.9228\n",
      "Epoch 10/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9261\n",
      "Epoch 10: val_loss did not improve from 0.26444\n",
      "244/244 [==============================] - 17s 69ms/step - loss: 0.2242 - acc: 0.9261 - val_loss: 0.2653 - val_acc: 0.9224\n",
      "Epoch 11/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2235 - acc: 0.9253\n",
      "Epoch 11: val_loss did not improve from 0.26444\n",
      "244/244 [==============================] - 17s 69ms/step - loss: 0.2238 - acc: 0.9252 - val_loss: 0.2651 - val_acc: 0.9228\n",
      "Epoch 12/100\n",
      "243/244 [============================>.] - ETA: 0s - loss: 0.2212 - acc: 0.9258\n",
      "Epoch 12: val_loss did not improve from 0.26444\n",
      "244/244 [==============================] - 17s 69ms/step - loss: 0.2214 - acc: 0.9257 - val_loss: 0.2645 - val_acc: 0.9228\n",
      "139/139 [==============================] - 2s 10ms/step\n",
      "0.7369532110161316 0.22651753350560347 0.9192307692307692 0.048\n"
     ]
    }
   ],
   "source": [
    "# Train and test the multimodal model for each embedding type and target problem\n",
    "\n",
    "embedding_types = ['word2vec', 'fasttext', 'concat']\n",
    "embedding_dict = [ner_word2vec, ner_fasttext, ner_concat]\n",
    "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
    "\n",
    "\n",
    "num_epoch = 100\n",
    "model_patience = 5\n",
    "monitor_criteria = 'val_loss'\n",
    "batch_size = 64\n",
    "iter_num = 2\n",
    "unit_sizes = [128, 256]\n",
    "#unit_sizes = [256]\n",
    "\n",
    "#layers = [\"LSTM\", \"GRU\"]\n",
    "layers = [\"GRU\"]\n",
    "for each_layer in layers:\n",
    "    print (\"Layer: \", each_layer)\n",
    "    for each_unit_size in unit_sizes:\n",
    "        print (\"Hidden unit: \", each_unit_size)\n",
    "\n",
    "        for embed_dict, embed_name in zip(embedding_dict, embedding_types):    \n",
    "            print (\"Embedding: \", embed_name)\n",
    "            print(\"=============================\")\n",
    "\n",
    "            temp_train_ner = dict((k, ner_word2vec[k]) for k in train_ids)\n",
    "            temp_dev_ner = dict((k, ner_word2vec[k]) for k in dev_ids)\n",
    "            temp_test_ner = dict((k, ner_word2vec[k]) for k in test_ids)\n",
    "\n",
    "            x_train_ner = create_dataset(temp_train_ner)\n",
    "            x_dev_ner = create_dataset(temp_dev_ner)\n",
    "            x_test_ner = create_dataset(temp_test_ner)\n",
    "\n",
    "\n",
    "            for iteration in range(1, iter_num):\n",
    "                print (\"Iteration number: \", iteration)\n",
    "\n",
    "                for each_problem in target_problems:\n",
    "                    print (\"Problem type: \", each_problem)\n",
    "                    print (\"__________________\")\n",
    "\n",
    "                    early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n",
    "                    best_model_name = \"avg-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.keras\"\n",
    "                    checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,\n",
    "                        save_best_only=True, mode='min')\n",
    "\n",
    "\n",
    "                    callbacks = [early_stopping_monitor, checkpoint]\n",
    "\n",
    "                    model = avg_ner_model(each_layer, each_unit_size, embed_name)\n",
    "                    \n",
    "                    model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1, \n",
    "                              validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, \n",
    "                              batch_size=batch_size )\n",
    "\n",
    "                    model.load_weights(best_model_name)\n",
    "\n",
    "                    probs, predictions = make_prediction_multi_avg(model, [x_test_lstm, x_test_ner])\n",
    "                    \n",
    "                    save_scores_multi_avg(predictions, probs, y_test[each_problem], \n",
    "                                embed_name, each_problem, iteration, each_unit_size, \n",
    "                                each_layer, type_of_ner)\n",
    "                    \n",
    "                    reset_keras(model)\n",
    "                    #del model\n",
    "                    clear_session()\n",
    "                    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show average over iterations for each category/embedding metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: GRU-128\n",
      "Embedding: word2vec\n",
      "                     AUROC            AUPRC         Accuracy               F1\n",
      "mort_hosp  0.8886 ± 0.0000  0.5962 ± 0.0000  0.9192 ± 0.0000  0.4863 ± 0.0000\n",
      "mort_icu   0.8909 ± 0.0000  0.5332 ± 0.0000  0.9410 ± 0.0000  0.4505 ± 0.0000\n",
      "los_3      0.7048 ± 0.0000  0.6456 ± 0.0000  0.6656 ± 0.0000  0.5617 ± 0.0000\n",
      "los_7      0.7356 ± 0.0000  0.2229 ± 0.0000  0.9195 ± 0.0000  0.0532 ± 0.0000\n",
      "\n",
      "Embedding: fasttext\n",
      "                     AUROC            AUPRC         Accuracy               F1\n",
      "mort_hosp  0.8844 ± 0.0000  0.5844 ± 0.0000  0.9188 ± 0.0000  0.4774 ± 0.0000\n",
      "mort_icu   0.8954 ± 0.0000  0.5430 ± 0.0000  0.9441 ± 0.0000  0.4711 ± 0.0000\n",
      "los_3      0.7042 ± 0.0000  0.6407 ± 0.0000  0.6609 ± 0.0000  0.5553 ± 0.0000\n",
      "los_7      0.7367 ± 0.0000  0.2166 ± 0.0000  0.9195 ± 0.0000  0.0378 ± 0.0000\n",
      "\n",
      "Embedding: concat\n",
      "                     AUROC            AUPRC         Accuracy               F1\n",
      "mort_hosp  0.8842 ± 0.0000  0.5911 ± 0.0000  0.9183 ± 0.0000  0.4835 ± 0.0000\n",
      "mort_icu   0.8942 ± 0.0000  0.5412 ± 0.0000  0.9439 ± 0.0000  0.4609 ± 0.0000\n",
      "los_3      0.7052 ± 0.0000  0.6415 ± 0.0000  0.6640 ± 0.0000  0.5634 ± 0.0000\n",
      "los_7      0.7393 ± 0.0000  0.2268 ± 0.0000  0.9195 ± 0.0000  0.0378 ± 0.0000\n",
      "\n",
      "Category: GRU-256\n",
      "Embedding: word2vec\n",
      "                     AUROC            AUPRC         Accuracy               F1\n",
      "mort_hosp  0.8836 ± 0.0000  0.5942 ± 0.0000  0.9179 ± 0.0000  0.4701 ± 0.0000\n",
      "mort_icu   0.8915 ± 0.0000  0.5308 ± 0.0000  0.9446 ± 0.0000  0.4864 ± 0.0000\n",
      "los_3      0.7040 ± 0.0000  0.6431 ± 0.0000  0.6647 ± 0.0000  0.5749 ± 0.0000\n",
      "los_7      0.7388 ± 0.0000  0.2314 ± 0.0000  0.9197 ± 0.0000  0.0220 ± 0.0000\n",
      "\n",
      "Embedding: fasttext\n",
      "                     AUROC            AUPRC         Accuracy               F1\n",
      "mort_hosp  0.8831 ± 0.0000  0.5933 ± 0.0000  0.9186 ± 0.0000  0.4444 ± 0.0000\n",
      "mort_icu   0.8923 ± 0.0000  0.5291 ± 0.0000  0.9434 ± 0.0000  0.4589 ± 0.0000\n",
      "los_3      0.7068 ± 0.0000  0.6429 ± 0.0000  0.6688 ± 0.0000  0.5712 ± 0.0000\n",
      "los_7      0.7370 ± 0.0000  0.2265 ± 0.0000  0.9192 ± 0.0000  0.0480 ± 0.0000\n",
      "\n",
      "Embedding: concat\n",
      "                     AUROC            AUPRC         Accuracy               F1\n",
      "mort_hosp  0.8852 ± 0.0000  0.5971 ± 0.0000  0.9197 ± 0.0000  0.4741 ± 0.0000\n",
      "mort_icu   0.8925 ± 0.0000  0.5400 ± 0.0000  0.9446 ± 0.0000  0.4754 ± 0.0000\n",
      "los_3      0.7098 ± 0.0000  0.6485 ± 0.0000  0.6649 ± 0.0000  0.5601 ± 0.0000\n",
      "los_7      0.7351 ± 0.0000  0.2299 ± 0.0000  0.9201 ± 0.0000  0.0275 ± 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define categories and metrics\n",
    "categories = [\"GRU-128\", \"GRU-256\"]\n",
    "metrics = {\"auc\":\"AUROC\", \"auprc\":\"AUPRC\", \"acc\":\"Accuracy\", \"F1\":\"F1\"}\n",
    "tasks = [\"mort_hosp\", \"mort_icu\", \"los_3\", \"los_7\"]\n",
    "embeddings = [\"word2vec\", \"fasttext\", \"concat\"]\n",
    "\n",
    "# Initialize dictionaries to store results\n",
    "results = {category: {embedding: {task: {metric_name: [] for metric, metric_name in metrics.items()} for task in tasks} for embedding in embeddings} for category in categories}\n",
    "\n",
    "# Directory where pickle files are stored\n",
    "directory = \"results/multimodal/\"\n",
    "\n",
    "# Loop through each file\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\"-new-avg-.p\"):\n",
    "        parts = filename.split(\"-\")\n",
    "        category = parts[0] + \"-\" + parts[1]\n",
    "        embedding = parts[2]\n",
    "        task = parts[3]\n",
    "        if category in categories and task in tasks and embedding in embeddings:\n",
    "            result_dict = pd.read_pickle(os.path.join(directory, filename))\n",
    "            for metric, metric_name in metrics.items():\n",
    "                results[category][embedding][task][metric_name].append(result_dict[metric])\n",
    "\n",
    "# Calculate average and standard deviation\n",
    "for category in categories:\n",
    "    print(f\"Category: {category}\")\n",
    "    for embedding in embeddings:\n",
    "        print(f\"Embedding: {embedding}\")\n",
    "        df_data = {task: {} for task in tasks}\n",
    "        for task in tasks:\n",
    "            task_data = {}\n",
    "            for metric, metric_name in metrics.items():\n",
    "                values = results[category][embedding][task][metric_name]\n",
    "                mean = np.mean(values)\n",
    "                std = np.std(values)\n",
    "                task_data[metric_name] = f\"{mean:.4f} \\u00B1 {std:.4f}\"\n",
    "            df_data[task] = task_data\n",
    "        df = pd.DataFrame(df_data).transpose()\n",
    "        print(df)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
