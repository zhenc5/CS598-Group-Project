{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81psNPGhIISC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHm0SLkOIRZy"
      },
      "outputs": [],
      "source": [
        "# read extracted timeseries features\n",
        "lvl2_train = pd.read_pickle(\"data/lvl2_imputer_train.pkl\")\n",
        "lvl2_dev =  pd.read_pickle(\"data/lvl2_imputer_dev.pkl\")\n",
        "lvl2_test =  pd.read_pickle(\"data/lvl2_imputer_test.pkl\")\n",
        "\n",
        "Ys =  pd.read_pickle(\"data/Ys.pkl\")\n",
        "Ys_train =  pd.read_pickle(\"data/Ys_train.pkl\")\n",
        "Ys_dev =  pd.read_pickle(\"data/Ys_dev.pkl\")\n",
        "Ys_test =  pd.read_pickle(\"data/Ys_test.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKuu3DJeITfI"
      },
      "outputs": [],
      "source": [
        "all_train_ids = set()\n",
        "for i in Ys_train.itertuples():\n",
        "    all_train_ids.add( i.Index[0] )\n",
        "\n",
        "all_dev_ids = set()\n",
        "for i in Ys_dev.itertuples():\n",
        "    all_dev_ids.add( i.Index[0] )\n",
        "\n",
        "all_test_ids = set()\n",
        "for i in Ys_test.itertuples():\n",
        "    all_test_ids.add( i.Index[0] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThrGucCkIUyY"
      },
      "outputs": [],
      "source": [
        "print (sum(Ys_train.mort_icu.values)*1.0 / len(Ys_train.mort_icu.values))\n",
        "print (sum(Ys_dev.mort_icu.values)*1.0 / len(Ys_dev.mort_icu.values))\n",
        "print (sum(Ys_test.mort_icu.values)*1.0 / len(Ys_test.mort_icu.values))\n",
        "print (\"====\")\n",
        "print (sum(Ys_train.mort_hosp.values)*1.0 / len(Ys_train.mort_hosp.values))\n",
        "print (sum(Ys_dev.mort_hosp.values)*1.0 / len(Ys_dev.mort_hosp.values))\n",
        "print (sum(Ys_test.mort_hosp.values)*1.0 / len(Ys_test.mort_hosp.values))\n",
        "print (\"====\")\n",
        "print (sum(Ys_train.los_3.values)*1.0 / len(Ys_train.los_3.values))\n",
        "print (sum(Ys_dev.los_3.values)*1.0 / len(Ys_dev.los_3.values))\n",
        "print (sum(Ys_test.los_3.values)*1.0 / len(Ys_test.los_3.values))\n",
        "print (\"====\")\n",
        "print (sum(Ys_train.los_7.values)*1.0 / len(Ys_train.los_7.values))\n",
        "print (sum(Ys_dev.los_7.values)*1.0 / len(Ys_dev.los_7.values))\n",
        "print (sum(Ys_test.los_7.values)*1.0 / len(Ys_test.los_7.values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLLj3RB9fqVC"
      },
      "source": [
        "### Create time series data from word2vec embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GITBIOCIWIg"
      },
      "outputs": [],
      "source": [
        "new_word2vec_dict = pd.read_pickle(\"data/new_ner_word2vec_dict.pkl\")\n",
        "new_keys = set(new_word2vec_dict.keys())\n",
        "new_train_ids = sorted(all_train_ids.intersection(new_keys))\n",
        "new_dev_ids = sorted(all_dev_ids.intersection(new_keys))\n",
        "new_test_ids = sorted(all_test_ids.intersection(new_keys))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5yUJk90fqVL"
      },
      "outputs": [],
      "source": [
        "# save train, dev, test ids\n",
        "type_of_ner = \"new\"\n",
        "pd.to_pickle(new_train_ids, \"data/\"+type_of_ner+\"_train_ids.pkl\")\n",
        "pd.to_pickle(new_dev_ids, \"data/\"+type_of_ner+\"_dev_ids.pkl\")\n",
        "pd.to_pickle(new_test_ids, \"data/\"+type_of_ner+\"_test_ids.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe8eHyUhIYpm"
      },
      "outputs": [],
      "source": [
        "data_ids = [(new_train_ids, new_dev_ids, new_test_ids)]\n",
        "data_names = [\"new\"]\n",
        "\n",
        "for i, (tr, de, te) in zip(data_names, data_ids):\n",
        "\n",
        "    y_train = Ys_train.loc[tr]\n",
        "    y_dev = Ys_dev.loc[de]\n",
        "    y_test = Ys_test.loc[te]\n",
        "\n",
        "    sub_train = lvl2_train.loc[tr]\n",
        "    sub_train = sub_train.loc[:, pd.IndexSlice[:, 'mean']]\n",
        "\n",
        "    sub_dev = lvl2_dev.loc[de]\n",
        "    sub_dev = sub_dev.loc[:, pd.IndexSlice[:, 'mean']]\n",
        "\n",
        "    sub_test = lvl2_test.loc[te]\n",
        "    sub_test = sub_test.loc[:, pd.IndexSlice[:, 'mean']]\n",
        "\n",
        "    sub_train = sub_train.values\n",
        "    sub_dev = sub_dev.values\n",
        "    sub_test = sub_test.values\n",
        "\n",
        "    # reshape the data for timeseries prediction\n",
        "    x_train_lstm = sub_train.reshape(int(sub_train.shape[0] / 24), 24, 104)\n",
        "    x_dev_lstm = sub_dev.reshape(int(sub_dev.shape[0] / 24), 24, 104)\n",
        "    x_test_lstm = sub_test.reshape(int(sub_test.shape[0] / 24), 24, 104)\n",
        "\n",
        "\n",
        "    pd.to_pickle(x_train_lstm, \"data/\"+i+\"_x_train.pkl\")\n",
        "    pd.to_pickle(x_dev_lstm, \"data/\"+i+\"_x_dev.pkl\")\n",
        "    pd.to_pickle(x_test_lstm, \"data/\"+i+\"_x_test.pkl\")\n",
        "\n",
        "    pd.to_pickle(y_train, \"data/\"+i+\"_y_train.pkl\")\n",
        "    pd.to_pickle(y_dev, \"data/\"+i+\"_y_dev.pkl\")\n",
        "    pd.to_pickle(y_test, \"data/\"+i+\"_y_test.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntoE7F0afqVL"
      },
      "outputs": [],
      "source": [
        "print(f\"Final cohort (# of patients/labels) using Word2Vec: {len(y_train) + len(y_dev) + len(y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WjU2K7jfqVL"
      },
      "source": [
        "### Create time series data from fast text embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw4uWoETfqVL"
      },
      "outputs": [],
      "source": [
        "new_fasttext_dict = pd.read_pickle(\"data/new_ner_fasttext_dict.pkl\")\n",
        "new_keys = set(new_fasttext_dict.keys())\n",
        "new_train_ids_ft = sorted(all_train_ids.intersection(new_keys))\n",
        "new_dev_ids_ft = sorted(all_dev_ids.intersection(new_keys))\n",
        "new_test_ids_ft = sorted(all_test_ids.intersection(new_keys))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iTka9HsfqVL"
      },
      "outputs": [],
      "source": [
        "# save train, dev, test ids\n",
        "type_of_ner = \"new\"\n",
        "pd.to_pickle(new_train_ids_ft, \"data/\"+type_of_ner+\"_train_ids_ft.pkl\")\n",
        "pd.to_pickle(new_dev_ids_ft, \"data/\"+type_of_ner+\"_dev_ids_ft.pkl\")\n",
        "pd.to_pickle(new_test_ids_ft, \"data/\"+type_of_ner+\"_test_ids_ft.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqvgpOpbfqVL"
      },
      "outputs": [],
      "source": [
        "data_ids = [(new_train_ids_ft, new_dev_ids_ft, new_test_ids_ft)]\n",
        "data_names = [\"new\"]\n",
        "\n",
        "for i, (tr, de, te) in zip(data_names, data_ids):\n",
        "\n",
        "    y_train = Ys_train.loc[tr]\n",
        "    y_dev = Ys_dev.loc[de]\n",
        "    y_test = Ys_test.loc[te]\n",
        "\n",
        "    sub_train = lvl2_train.loc[tr]\n",
        "    sub_train = sub_train.loc[:, pd.IndexSlice[:, 'mean']]\n",
        "\n",
        "    sub_dev = lvl2_dev.loc[de]\n",
        "    sub_dev = sub_dev.loc[:, pd.IndexSlice[:, 'mean']]\n",
        "\n",
        "    sub_test = lvl2_test.loc[te]\n",
        "    sub_test = sub_test.loc[:, pd.IndexSlice[:, 'mean']]\n",
        "\n",
        "    sub_train = sub_train.values\n",
        "    sub_dev = sub_dev.values\n",
        "    sub_test = sub_test.values\n",
        "\n",
        "    # reshape the data for timeseries prediction\n",
        "    x_train_lstm = sub_train.reshape(int(sub_train.shape[0] / 24), 24, 104)\n",
        "    x_dev_lstm = sub_dev.reshape(int(sub_dev.shape[0] / 24), 24, 104)\n",
        "    x_test_lstm = sub_test.reshape(int(sub_test.shape[0] / 24), 24, 104)\n",
        "\n",
        "\n",
        "    pd.to_pickle(x_train_lstm, \"data/\"+i+\"_x_train_ft.pkl\")\n",
        "    pd.to_pickle(x_dev_lstm, \"data/\"+i+\"_x_dev_ft.pkl\")\n",
        "    pd.to_pickle(x_test_lstm, \"data/\"+i+\"_x_test_ft.pkl\")\n",
        "\n",
        "    pd.to_pickle(y_train, \"data/\"+i+\"_y_train_ft.pkl\")\n",
        "    pd.to_pickle(y_dev, \"data/\"+i+\"_y_dev_ft.pkl\")\n",
        "    pd.to_pickle(y_test, \"data/\"+i+\"_y_test_ft.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCxZKw88fqVL"
      },
      "outputs": [],
      "source": [
        "print(f\"Final cohort (# of patients/labels using Fast Text): {len(y_train) + len(y_dev) + len(y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryiYXh-zfqVL"
      },
      "source": [
        "### create time series data from concatenated embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceUKUM3JfqVL"
      },
      "outputs": [],
      "source": [
        "new_concat_dict = pd.read_pickle(\"data/new_ner_combined_dict.pkl\")\n",
        "new_keys = set(new_concat_dict.keys())\n",
        "new_train_ids_concat = sorted(all_train_ids.intersection(new_keys))\n",
        "new_dev_ids_concat = sorted(all_dev_ids.intersection(new_keys))\n",
        "new_test_ids_concat = sorted(all_test_ids.intersection(new_keys))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3oFP-MBfqVM"
      },
      "outputs": [],
      "source": [
        "# save train, dev, test ids\n",
        "type_of_ner = \"new\"\n",
        "pd.to_pickle(new_train_ids_concat, \"data/\"+type_of_ner+\"_train_ids_concat.pkl\")\n",
        "pd.to_pickle(new_dev_ids_concat, \"data/\"+type_of_ner+\"_dev_ids_concat.pkl\")\n",
        "pd.to_pickle(new_test_ids_concat, \"data/\"+type_of_ner+\"_test_ids_concat.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4i3Rnh9fqVM"
      },
      "outputs": [],
      "source": [
        "data_ids = [(new_train_ids_concat, new_dev_ids_concat, new_test_ids_concat)]\n",
        "data_names = [\"new\"]\n",
        "\n",
        "for i, (tr, de, te) in zip(data_names, data_ids):\n",
        "\n",
        "    y_train = Ys_train.loc[tr]\n",
        "    y_dev = Ys_dev.loc[de]\n",
        "    y_test = Ys_test.loc[te]\n",
        "\n",
        "    sub_train = lvl2_train.loc[tr]\n",
        "    sub_train = sub_train.loc[:, pd.IndexSlice[:, 'mean']]\n",
        "\n",
        "    sub_dev = lvl2_dev.loc[de]\n",
        "    sub_dev = sub_dev.loc[:, pd.IndexSlice[:, 'mean']]\n",
        "\n",
        "    sub_test = lvl2_test.loc[te]\n",
        "    sub_test = sub_test.loc[:, pd.IndexSlice[:, 'mean']]\n",
        "\n",
        "    sub_train = sub_train.values\n",
        "    sub_dev = sub_dev.values\n",
        "    sub_test = sub_test.values\n",
        "\n",
        "    # reshape the data for timeseries prediction\n",
        "    x_train_lstm = sub_train.reshape(int(sub_train.shape[0] / 24), 24, 104)\n",
        "    x_dev_lstm = sub_dev.reshape(int(sub_dev.shape[0] / 24), 24, 104)\n",
        "    x_test_lstm = sub_test.reshape(int(sub_test.shape[0] / 24), 24, 104)\n",
        "\n",
        "\n",
        "    pd.to_pickle(x_train_lstm, \"data/\"+i+\"_x_train_concat.pkl\")\n",
        "    pd.to_pickle(x_dev_lstm, \"data/\"+i+\"_x_dev_concat.pkl\")\n",
        "    pd.to_pickle(x_test_lstm, \"data/\"+i+\"_x_test_concat.pkl\")\n",
        "\n",
        "    pd.to_pickle(y_train, \"data/\"+i+\"_y_train_concat.pkl\")\n",
        "    pd.to_pickle(y_dev, \"data/\"+i+\"_y_dev_concat.pkl\")\n",
        "    pd.to_pickle(y_test, \"data/\"+i+\"_y_test_concat.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXKnGxpgfqVM"
      },
      "outputs": [],
      "source": [
        "print(f\"Final cohort (# of patients/labels using concatenated embeddings): {len(y_train) + len(y_dev) + len(y_test)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}