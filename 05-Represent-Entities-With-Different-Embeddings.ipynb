{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nHtYS16HN8s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec, FastText\n",
        "#import glove\n",
        "#from glove import Corpus\n",
        "\n",
        "import collections\n",
        "import gc\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaXWFg5xRP68"
      },
      "source": [
        "### Process NER clinical notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q5J-bJQRP69"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "new_notes = pd.read_pickle(\"data/ner_df.p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmQdN4XQHeAu"
      },
      "outputs": [],
      "source": [
        "# Filter out rows with empty 'ner' values\n",
        "null_index_list = [i.Index for i in new_notes.itertuples() if len(i.ner) == 0]\n",
        "new_notes.drop(null_index_list, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE3iEeTgHflr"
      },
      "outputs": [],
      "source": [
        "# Process NER data\n",
        "med7_ner_data = {}\n",
        "for ii in new_notes.itertuples():\n",
        "    p_id = ii.SUBJECT_ID\n",
        "    ind = ii.Index\n",
        "\n",
        "    try:\n",
        "        new_ner = new_notes.loc[ind].ner\n",
        "    except:\n",
        "        new_ner = []\n",
        "\n",
        "    new_temp = [(k[0], k[1]) for j in new_ner for k in j]\n",
        "\n",
        "    if p_id in med7_ner_data:\n",
        "        med7_ner_data[p_id].extend(new_temp)\n",
        "    else:\n",
        "        med7_ner_data[p_id] = new_temp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODo5KNAdHhdF"
      },
      "outputs": [],
      "source": [
        "# Save processed NER data\n",
        "pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqf2mXC7RP69"
      },
      "outputs": [],
      "source": [
        "data_types = [med7_ner_data]\n",
        "data_names = [\"new_ner\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVqMyGsNRP69"
      },
      "source": [
        "### Represent medical entites with Word2Vec embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgoTWjPQHcvw"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "w2vec = Word2Vec.load(\"embeddings/word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtLIccAaHjmD"
      },
      "outputs": [],
      "source": [
        "# Calculate mean for vectors\n",
        "def mean(a):\n",
        "    return sum(a) / len(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHfO9OOAHlCj"
      },
      "outputs": [],
      "source": [
        "# Process and save Word2Vec embeddings\n",
        "\n",
        "for data, names in zip(data_types, data_names):\n",
        "    new_word2vec = {}\n",
        "    print(\"w2vec starting..\")\n",
        "    for k, v in data.items():\n",
        "        patient_temp = []\n",
        "        if isinstance(v, list):\n",
        "            for i in v:\n",
        "                if isinstance(i, tuple) and len(i) == 2 and isinstance(i[0], str):\n",
        "                    if i[0] in w2vec.wv:\n",
        "                        patient_temp.append(w2vec.wv[i[0]])\n",
        "                    elif len(i[0].split(\" \")) > 1:\n",
        "                        avg = []\n",
        "                        words = i[0].split(\" \")\n",
        "                        num = 0\n",
        "                        for each_word in words:\n",
        "                            if each_word in w2vec.wv:\n",
        "                                temp = w2vec.wv[each_word]\n",
        "                                avg.append(temp)\n",
        "                                num += 1\n",
        "                        if num > 0:\n",
        "                            avg = np.asarray(avg)\n",
        "                            t = np.asarray(list(map(mean, zip(*avg))))\n",
        "                            patient_temp.append(t)\n",
        "        if patient_temp:\n",
        "            new_word2vec[k] = patient_temp\n",
        "\n",
        "    print(f\"Number of Word2Vec embeddings: {len(new_word2vec)}\")\n",
        "    pd.to_pickle(new_word2vec, \"data/\"+names+\"_word2vec_dict.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q33Rl99SRP6_"
      },
      "source": [
        "### Represent medical entites with FastText embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhB8bS6URP6_"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "fasttext = FastText.load(\"embeddings/fasttext.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyn1jYcxRP6_"
      },
      "outputs": [],
      "source": [
        "# Process and save FastText embeddings\n",
        "\n",
        "for data, names in zip(data_types, data_names):\n",
        "    new_fasttextvec = {}\n",
        "    print(\"fasttext starting..\")\n",
        "\n",
        "    for k,v in data.items():\n",
        "        patient_temp = []\n",
        "        for i in v:\n",
        "            try:\n",
        "                patient_temp.append(fasttext.wv[i[0]])\n",
        "            except:\n",
        "                pass\n",
        "        if len(patient_temp) == 0: continue\n",
        "        new_fasttextvec[k] = patient_temp\n",
        "\n",
        "    print(f\"Number of Fasttext embeddings: {len(new_fasttextvec)}\")\n",
        "    pd.to_pickle(new_fasttextvec, \"data/\"+names+\"_fasttext_dict.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4fEWm5cRP6_"
      },
      "source": [
        "### Represent medical entites with combined Word2Vec + FastText embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnmZSk3ARP6_"
      },
      "outputs": [],
      "source": [
        "# Process and save concatenated embeddings\n",
        "\n",
        "for data, names in zip(data_types, data_names):\n",
        "    print(\"combined starting..\")\n",
        "    new_concatvec = {}\n",
        "\n",
        "    for k,v in data.items():\n",
        "        patient_temp = []\n",
        "    #     if k != 6: continue\n",
        "        for i in v:\n",
        "            w2vec_temp = []\n",
        "            try:\n",
        "                w2vec_temp = w2vec.wv[i[0]]\n",
        "            except:\n",
        "                avg = []\n",
        "                num = 0\n",
        "                temp = []\n",
        "\n",
        "                if len(i[0].split(\" \")) > 1:\n",
        "                    for each_word in i[0].split(\" \"):\n",
        "                        try:\n",
        "                            temp = w2vec.wv[each_word]\n",
        "                            avg.append(temp)\n",
        "                            num += 1\n",
        "                        except:\n",
        "                            pass\n",
        "                    if num == 0:\n",
        "                        w2vec_temp = [0] * 100\n",
        "                    else:\n",
        "                        #print(f\"i: {i[0]}\")\n",
        "                        avg = np.asarray(avg)\n",
        "                        #w2vec_temp = np.asarray(map(mean, zip(*avg)))\n",
        "                        w2vec_temp = np.mean(avg, axis=0)\n",
        "                        #print(f\"w2v: {w2vec_temp}\")\n",
        "                else:\n",
        "                    w2vec_temp = [0] * 100\n",
        "            try:\n",
        "                fasttemp = fasttext.wv[i[0]]\n",
        "            except:\n",
        "                fasttemp = [0] * 100\n",
        "\n",
        "            #print(f\"i[0]: {i[0]}\")\n",
        "            #print(f\"Length of w2v: {len(w2vec_temp)}\")\n",
        "            #print(f\"Length of fasttext: {len(fasttemp)}\")\n",
        "            appended = np.append(fasttemp, w2vec_temp, 0)\n",
        "            patient_temp.append(appended)\n",
        "        if len(patient_temp) == 0: continue\n",
        "        new_concatvec[k] = patient_temp\n",
        "\n",
        "    print(f\"Number of concatenated embeddings: {len(new_concatvec)}\")\n",
        "    pd.to_pickle(new_concatvec, \"data/\"+names+\"_combined_dict.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5-hPqA2RP6_"
      },
      "source": [
        "### Standardize all 3 embeddings\n",
        "Remove key-value pairs from fasttext embeddings and combined embeddings for keys that are not present in word2vec embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeMnqLJjRP6_"
      },
      "outputs": [],
      "source": [
        "new_fasttext_dict = new_fasttextvec.copy()\n",
        "new_word2vec_dict =  new_word2vec.copy()\n",
        "new_combined_dict = new_concatvec.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBSQIOitRP6_"
      },
      "outputs": [],
      "source": [
        "diff = set(new_fasttext_dict.keys()).difference(set(new_word2vec_dict))\n",
        "for i in diff:\n",
        "    del new_fasttext_dict[i]\n",
        "    del new_combined_dict[i]\n",
        "print (len(new_word2vec_dict), len(new_fasttext_dict), len(new_combined_dict))\n",
        "\n",
        "pd.to_pickle(new_word2vec_dict, \"data/\"+\"new_ner\"+\"_word2vec_limited_dict.pkl\")\n",
        "pd.to_pickle(new_fasttext_dict, \"data/\"+\"new_ner\"+\"_fasttext_limited_dict.pkl\")\n",
        "pd.to_pickle(new_combined_dict, \"data/\"+\"new_ner\"+\"_combined_limited_dict.pkl\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}